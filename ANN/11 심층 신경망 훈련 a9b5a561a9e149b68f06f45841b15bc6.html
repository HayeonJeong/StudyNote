<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>11. 심층 신경망 훈련</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="a9b5a561-a9e1-49b6-8f06-f45841b15bc6" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">🧠</span></div><h1 class="page-title">11. 심층 신경망 훈련</h1><p class="page-description"></p><table class="properties"><tbody><tr class="property-row property-row-created_time"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesCreatedAt"><path d="M8 15.126C11.8623 15.126 15.0615 11.9336 15.0615 8.06445C15.0615 4.20215 11.8623 1.00293 7.99316 1.00293C4.13086 1.00293 0.938477 4.20215 0.938477 8.06445C0.938477 11.9336 4.1377 15.126 8 15.126ZM8 13.7383C4.85547 13.7383 2.33301 11.209 2.33301 8.06445C2.33301 4.91992 4.84863 2.39746 7.99316 2.39746C11.1377 2.39746 13.6738 4.91992 13.6738 8.06445C13.6738 11.209 11.1445 13.7383 8 13.7383ZM4.54102 8.91211H7.99316C8.30078 8.91211 8.54004 8.67285 8.54004 8.37207V3.8877C8.54004 3.58691 8.30078 3.34766 7.99316 3.34766C7.69238 3.34766 7.45312 3.58691 7.45312 3.8877V7.83203H4.54102C4.2334 7.83203 4.00098 8.06445 4.00098 8.37207C4.00098 8.67285 4.2334 8.91211 4.54102 8.91211Z"></path></svg></span>Created time</th><td><time>@May 2, 2024 9:24 PM</time></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesSelect"><path d="M8 15.126C11.8623 15.126 15.0615 11.9336 15.0615 8.06445C15.0615 4.20215 11.8623 1.00293 7.99316 1.00293C4.13086 1.00293 0.938477 4.20215 0.938477 8.06445C0.938477 11.9336 4.1377 15.126 8 15.126ZM8 13.7383C4.85547 13.7383 2.33301 11.209 2.33301 8.06445C2.33301 4.91992 4.84863 2.39746 7.99316 2.39746C11.1377 2.39746 13.6738 4.91992 13.6738 8.06445C13.6738 11.209 11.1445 13.7383 8 13.7383ZM7.62402 10.6348C7.79492 10.915 8.20508 10.9287 8.37598 10.6348L10.666 6.73145C10.8574 6.41016 10.7002 6.04102 10.3652 6.04102H5.62793C5.29297 6.04102 5.14941 6.43066 5.32031 6.73145L7.62402 10.6348Z"></path></svg></span>Subject</th><td><span class="selected-value select-value-color-purple">인공신경망</span></td></tr></tbody></table></header><div class="page-body"><nav id="edd3101d-68c9-444f-a299-2c86991b3f1e" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#5c6a99f3-542d-49db-b920-b61bd0149903">11.1 Gradient 소실, 폭주 문제</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#9c19024d-a9fe-4b87-8089-2d169d1d2835">11.1.1 글로럿 He 초기화</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#2f3f4430-1841-4af8-9c0a-350b46c74297">11.1.2 고급 활성화 함수</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#af2d9be0-d0a9-4d61-b473-838ccc677047">LeakyReLU</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#87ac4f49-ce03-4f4a-98ac-25b81ca6074b">ELU와 SELU</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#3b0ceeae-a1ea-4f5a-b193-b3de6ce252bf">GELU, Swish, Mish</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#cd23d0dd-849e-4378-b8c3-c96d283424dc">11.1.3 배치 정규화</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#a3f7de3d-1b02-49ac-98ae-6882538042e4">케라스로 배치 정규화 구현하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1900e87c-622a-419b-84d2-e34bebbd952f">11.1.4 그레디언트 클리핑_gradient clipping</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#5c21bea2-10d0-4b23-804a-4f475bda9aca">11.2 사전 훈련된 층 재사용하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#cbff207b-f2b7-44e2-9eb8-deddff5b2723">11.2.1 케라스를 사용한 전이 학습</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#f650c634-75fd-4568-b79f-d9a70127efe7">11.2.2 비지도 사전 훈련_unsupervised pretraining</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#d8d8b09e-7979-4272-b409-9f86fd33599e">11.2.3 보조 작업에서 사전 훈련</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#25c9e05c-bcf4-4536-afde-1e467a567338">11.3 고속 옵티마이저</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#574efcdd-3de2-4dc4-ae09-1fccba9e1e60">11.3.1 모멘텀 최적화</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#1c977735-1be6-46c2-a599-21cbb842fbb4">11.3.2 네스테로프 가속 경사</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#30363aa8-43c2-43d8-a641-07d8ef2435bc">11.3.3 AdaGrad</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c0e6c56c-883b-418e-9df5-abb8678cff72">11.3.4 RMSProp (AdaGrad의 변형)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#bafd4bfd-76ea-42b0-b9d0-7b5d7e78d7c2">11.3.5 Adam (모멘텀 최적화 + RMSProp)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#35b6bed1-f01b-43c2-964a-0719d460654e">11.3.6 AdaMax (Adam의 변형)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#6b439a2a-9295-40f5-a36f-b4907d74990c">11.3.7 Nadam (Adam의 변형)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#334c409e-540e-486d-907c-ad512bd48c97">11.3.8 AdamW (Adam의 변형)</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#68ed850b-3b5b-4eef-8f52-9c17bbb949e1">11.3.9 학습률 스케줄링</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#0db977ac-1a5a-4ffe-9b7c-003ed758c94b">1) 거듭제곱 기반 스케줄링_power scheduling</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#da8da119-9da3-45d3-ba65-0c805a9a4ed4">2) 지수 기반 스케줄링_exponential scheduling</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#323219fa-f8b8-4c23-a278-0f119bb848fe">3) 구간별 고정 스케줄링_piecewise constant scheduling</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#77414319-6bff-4a4f-a77b-631fc524042e">4) 성능 기반 스케줄링_performance scheduling</a></div><div class="table_of_contents-item table_of_contents-indent-2"><a class="table_of_contents-link" href="#cbfa3db0-2d37-4002-b2b1-4fbef7fc4d80">5) 1사이클 스케줄링</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#1b46b2ee-f810-468b-831e-1d3c703f4ab3">11.4 규제를 사용해 과대적합 피하기</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#88542ed8-03dd-4282-873c-9909cb6da553">11.4.1 l1과 l2 규제</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#3aa9965a-f8d3-4132-8576-7404cb716461">11.4.2 드롭아웃</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#d7cbdf45-16e9-4ab0-9000-18b40f5db6f9">11.4.3 몬테 카를로 드롭아웃</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#0e3c9090-f399-4331-af43-75bb8072b0d4">11.4.4 맥스-노름 규제</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c3c4a591-5128-4c47-b575-1532412fc82a">11.5 요약 및 실용적인 가이드라인</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#4193289d-a861-4263-8ec8-a2a8807ae850">연습문제</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#65c8e0e8-1ed4-475b-8ff3-315e2430e704">1) 글로럿 초기화와 He 초기화가 해결하고자 하는 문제는 무엇인가요?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#db55ae89-bf25-470e-a3ab-1ffe3bef794c">2) He 초기화를 사용하여 랜덤으로 선택한 값이라면 모든 가중치를 같은 값으로 초기화해도 괜찮을까요?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#cce6fc89-090f-4e64-bdd3-4d4dcdcac357">3) 편향을 0으로 초기화해도 괜찮을까요?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c32a3fa4-2deb-4ef7-9a95-9faecc6f94cf">4) 어떤 경우에 이 장에서 언급한 활성화 함수를 사용하나요?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#5e7c408c-12c3-4535-8773-59e1fb6b9b58">5) SGD 옵티마이저를 사용할 때 momentum 하이퍼파라미터를 너무 1에 가깝게 하면(ex. 0/99999) 어떤 일이 일어날까요?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#d5a4b61c-647f-4f05-a21f-db68d6b3568d">6) 희소 모델을 만들 수 있는 3가지 방법은 무엇인가요?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#3ebdb317-e1df-405b-b228-0ce6bb556f90">7) 드롭아웃이 훈련 속도를 느리게 만드나요? 추론(새로운 샘플에 대한 예측을 만드는 것)도 느리게 만드나요? MC 드롭아웃은 어떤가요?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#2c9c8fce-976b-40b5-955c-7eebf61b2136">8) CIFAR10 이미지 데이터셋에 심층 신경망을 훈련해보세요.</a></div></nav><hr id="b7c9a50f-b61f-4486-a872-af543a182fbe"/><p id="46068e60-6e00-46a2-8fc0-52d02ff7ffa1" class="">복잡한 문제를 다룰 경우, 수백 개의 뉴런으로 구성된 10개 이상의 층을 수십만 개의 가중치로 연결해 깊은 인공 신경망을 훈련해야 할 것이다. 이러한 심층 신경망을 훈련하는 데 여러가지 문제가 생길 수 있다.</p><table id="c2720d16-18e7-413f-9427-448ae378e97c" class="simple-table"><tbody><tr id="eaf37b58-892f-488d-91d2-2ad45a069fde"><td id="Ir&gt;=" class="block-color-blue_background" style="width:342.9965515136719px">문제점</td><td id="X:lk" class="block-color-blue_background" style="width:330.9965515136719px">11장에서 알아볼 해결책</td></tr><tr id="b501946f-5e05-4aef-8759-a2253c3eeaf8"><td id="Ir&gt;=" class="" style="width:342.9965515136719px">출력층에서 멀어질수록 gradient가 점점 작아지는 ‘소실’, 커지는 문제인 ‘폭주’가 나타날 수 있다. 두 문제 모두 하위 층을 훈련하기 매우 어렵게 만든다.</td><td id="X:lk" class="" style="width:330.9965515136719px">해결 방법</td></tr><tr id="dc729558-112d-4dd2-915e-ee34065c542c"><td id="Ir&gt;=" class="" style="width:342.9965515136719px">훈련 데이터가 충분하지 않은 경우, 레이블 제작 비용이 많이 드는 경우</td><td id="X:lk" class="" style="width:330.9965515136719px">레이블된 데이터가 적을 때 복잡한 문제를 다루는 데 도움이 되는 ‘전이 학습’과 ‘비지도 사전 훈련’을 알아본다.</td></tr><tr id="09322a67-2bca-4c44-847c-69b3af8c1e57"><td id="Ir&gt;=" class="" style="width:342.9965515136719px">훈련이 극단적으로 느려질 수 있다.</td><td id="X:lk" class="" style="width:330.9965515136719px">훈련 속도를 크게 높여주는 다양한 최적화 방법</td></tr><tr id="922e00da-dbc5-4737-8722-6b8ad4084099"><td id="Ir&gt;=" class="" style="width:342.9965515136719px">과대적합<br/>1. 너무 많은 파라미터를 가진 모델<br/>2. 훈련 샘플이 충분하지 않은 경우<br/>3. 잡음이 많은 경우<br/></td><td id="X:lk" class="" style="width:330.9965515136719px">규제 기법</td></tr></tbody></table><h1 id="5c6a99f3-542d-49db-b920-b61bd0149903" class="">11.1 Gradient 소실, 폭주 문제</h1><ul id="1a01867e-a277-4b1f-8821-f61914aa0c1f" class="bulleted-list"><li style="list-style-type:disc">10장 내용<ul id="05709a91-94e2-4d1f-a168-11ebb1c8bf1a" class="bulleted-list"><li style="list-style-type:circle">Back propagation의 두 번째 단계는 출력층에서 입력층으로 ‘오차 그레디언트’를 전파하면서 진행된다.</li></ul><ul id="45a3d8f5-78c0-4e61-9a0f-cff96a07712f" class="bulleted-list"><li style="list-style-type:circle">알고리즘이 신경망의 모든 파라미터에 대한 오차 함수의 gradient를 계산하면, 경사하강법 단계에서 이 gradient를 사용하여 각 파라미터를 수정한다.</li></ul></li></ul><ul id="e247a9f9-2fbb-49d4-be51-51365bd0a070" class="bulleted-list"><li style="list-style-type:disc"><strong>그레디언트 소실_Vanishing Gradient</strong><ul id="38aaa010-16be-43b6-954b-872da973b56f" class="bulleted-list"><li style="list-style-type:circle">알고리즘이 하위층으로 진행될수록 gradient가 점점 작아지는 경우</li></ul><ul id="71816d35-aeb9-433f-8591-6212fb5afd09" class="bulleted-list"><li style="list-style-type:circle">이러한 경우에 하위 층의 연결 가중치가 변경되지 않는데, 훈련이 좋은 솔루션으로 수렴되지 않을 것이다.</li></ul></li></ul><ul id="94f83d11-1c97-451f-886d-bf21b60850ca" class="bulleted-list"><li style="list-style-type:disc"><strong>그레디언트 폭주_Exploding Gradient</strong><ul id="bd624bc7-7e59-409a-ab30-eb55685aff8e" class="bulleted-list"><li style="list-style-type:circle">gradient가 점점 커져서 여러층이 비정상적으로 큰 가중치르 갱신되면 알고리즘은 발산(diverse)한다.</li></ul></li></ul><ul id="bdcdf117-225d-4ed4-80a3-eaa9f9a39ec2" class="bulleted-list"><li style="list-style-type:disc">이런식으로 불안정한 gradient는 심층신경망 훈련을 어렵게 만든다.</li></ul><hr id="920a15e0-ca3f-4793-923f-1bd1ad357870"/><ul id="a18f8a76-9e65-46e0-a51d-3360a33f7cd2" class="bulleted-list"><li style="list-style-type:disc">이렇게 gradient를 불안정하게 하는 원인은?<ul id="b7cd3ed8-9945-4d03-9fca-c81aaada662d" class="bulleted-list"><li style="list-style-type:circle">logistic sigmoid 활성화 함수 + 가중치 초기화 방식 (평균이 0이고 표준편차가 1인 정규분포)</li></ul><ul id="163c3d41-7a9b-4ced-8086-668f22988608" class="bulleted-list"><li style="list-style-type:circle">이 활성화 함수와 초기화 방식을 사용했을 때, 각 층에서 출력의 분산이 입력의 분산보다 더 크다.</li></ul><ul id="b22b74e9-8185-4c16-a4a7-ac40bbcddb46" class="bulleted-list"><li style="list-style-type:circle">신경망 위쪽으로 갈수록 층을 지날 때마다 분산이 계속 커져, 가장 높은 층에서는 활성화 함수가 0이나 1로 수렴한다.<figure id="857d09af-ae5c-41f8-a342-a27707909367" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled.png"><img style="width:432px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled.png"/></a></figure><ul id="bacc087c-e2b5-4ea6-a795-63708388bc3e" class="bulleted-list"><li style="list-style-type:square">sigmoid 함수의 평균이 0이 아니라 0.5. (항상 양수를 출력하니까 출력의 가중치 합이 입력보다 계속 커지는 것이다. = 편향 이동(bias shift))<ul id="bc0b4803-20e1-48d4-b56b-bf91d65e0460" class="bulleted-list"><li style="list-style-type:disc">tanh 함수는 평균이 0이므로 sigmoid 함수보다 조금 더 낫다.</li></ul></li></ul><ul id="093d018b-0bbb-4622-8fe1-394b13fed096" class="bulleted-list"><li style="list-style-type:square">역전파가 될 때, 조금 있는 gradient는 최상위 층에서부터 역전파가 진행되면서 점차 약해져서 실제로 아래쪽 층에는 아무것도 도달하지 않게 된다.</li></ul></li></ul></li></ul><h2 id="9c19024d-a9fe-4b87-8089-2d169d1d2835" class="">11.1.1 글로럿 He 초기화</h2><ul id="6bb40990-afc1-42e5-88d3-2c26c5eca3e1" class="bulleted-list"><li style="list-style-type:disc">글로럿과 벤지오의 논문: 불안정한 그레디언트 문제를 완화하는 방법<ul id="af4ccb18-88b9-4d77-b915-b06bb095fc54" class="bulleted-list"><li style="list-style-type:circle">각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야. (fan-in = fan-out)<ul id="12c43156-ef50-4e40-8ae2-ce86c718b2e2" class="bulleted-list"><li style="list-style-type:square">fan-in (팬 인): 입력 개수</li></ul><ul id="b825d2cc-3aaa-48db-a6f0-0f9edbd60f21" class="bulleted-list"><li style="list-style-type:square">fan-out (팬 아웃): 출력 개수</li></ul></li></ul><ul id="9b81139e-6318-4c5a-b17a-fd342342bd9a" class="bulleted-list"><li style="list-style-type:circle">역방향에서 층을 통과하기 전과 후의 그레디언트 분산이 동일해야.</li></ul></li></ul><ul id="63f73a90-2c94-43f0-a1a1-facc0f0229f6" class="bulleted-list"><li style="list-style-type:disc">글로럿 초기화_Glorot initialization = 세이비어 초기화_Xavier initialization<ul id="ac87a392-5da8-4c7c-af31-a0fd8af274ee" class="bulleted-list"><li style="list-style-type:circle">각 층의 연결 가중치를 랜덤으로 초기화<figure id="07b1728b-c18a-488c-9b2b-e773c98e4b2e" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%201.png"><img style="width:144px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%201.png"/></a></figure><figure id="194c0c94-8dc6-4c9c-a709-b28d1962b96c" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%202.png"><img style="width:288px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%202.png"/></a></figure></li></ul></li></ul><ul id="7bc4d893-4642-454a-b4a5-a6d50267c2bc" class="bulleted-list"><li style="list-style-type:disc">르쿤 초기화_LeCun initialization: fan_avg를 fan_in으로 바꾸기<ul id="d00c988e-20ad-4fdd-b085-cb2084e538ff" class="bulleted-list"><li style="list-style-type:circle">fan_in = fan_out이면 르쿤 초기화가 글로럿 초기화와 동일</li></ul><ul id="17cb54d9-b213-4d3d-8731-08d5eedaa14e" class="bulleted-list"><li style="list-style-type:circle">훈련 속도를 높일 수 있다.</li></ul></li></ul><ul id="bf8960a6-3ded-4274-9f0d-90bbd56ffb8c" class="bulleted-list"><li style="list-style-type:disc">He 초기화 = 카이밍 초기화_Kaiming initialization<ul id="53f955c2-0b01-43d0-8292-18f9cfc14777" class="bulleted-list"><li style="list-style-type:circle">ReLU 함수 사용</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="4643411e-8aba-4859-a27d-ceeacf0671eb" class="code"><code class="language-Python">import tensorflow as tf

dense = tf.kreas.layers.Dense(50, activation=&quot;relu&quot;,
															kernel_initializer=&quot;he_normal&quot;)</code></pre><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="32f09506-7b4b-4771-8a78-a35bda2d2888" class="code"><code class="language-Python">he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=&quot;fan_avg&quot;,
																										distribution=&quot;uniform&quot;)

dense = tf.kreas.layers.Dense(50, activation=&quot;sigmoid&quot;,
															kernel_initializer=he_avg_init)</code></pre></li></ul><hr id="c8befb0b-7038-4d82-9737-1236727faee7"/><figure id="e2c42d9d-402f-415c-9532-52de622be908" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%203.png"><img style="width:720px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%203.png"/></a></figure><ul id="1479956f-952a-4454-a25b-e60f538195b5" class="bulleted-list"><li style="list-style-type:disc">케라스는 기본적으로 균등 분포의 글로럿 초기화를 사용.</li></ul><h2 id="2f3f4430-1841-4af8-9c0a-350b46c74297" class="">11.1.2 고급 활성화 함수</h2><ul id="d3557785-e0df-44da-a46b-4a53f44012b9" class="bulleted-list"><li style="list-style-type:disc">활성화 함수를 잘못 선택하면, 자칫 그레디언트 소실이나 폭주로 이어질 수 있다.</li></ul><ul id="5efc6623-7b6c-42dd-a25c-296c3ab0e52a" class="bulleted-list"><li style="list-style-type:disc">ReLU 함수는 뉴런의 방식과 비슷한 시그모이드 함수보다 심층 신경망에서 더 잘 작동함을 알아보았다.</li></ul><ul id="cebd4193-0368-4987-b6be-67809371afc0" class="bulleted-list"><li style="list-style-type:disc">하지만, ReLU는 일부 뉴런이 0 이외의 값을 출력하지 않아 완벽하지 않다.<ul id="a4440cbe-3b23-4e40-8d91-c66e467b9298" class="bulleted-list"><li style="list-style-type:circle">특히, 큰 학습률을 사용하면 신경망의 뉴런 절반이 죽어 있기도 하다.</li></ul><ul id="623dec1a-4d6e-4918-9915-7e0285707d58" class="bulleted-list"><li style="list-style-type:circle">뉴런의 가중치가 바뀌어 훈련 세트에 있는 모든 샘플에 대해 ReLU 함수의 입력이 음수가 되면 뉴런이 죽게 된다.</li></ul></li></ul><ul id="e3fb9375-d6ea-4146-a86c-699a14bc5778" class="bulleted-list"><li style="list-style-type:disc">그래서 아래와 같은 ReLU 함수의 변형을 사용하게 된다.</li></ul><h3 id="af2d9be0-d0a9-4d61-b473-838ccc677047" class="">LeakyReLU</h3><figure id="db6140d6-41a6-4d61-9f07-0f48c5604352" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%204.png"><img style="width:432px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%204.png"/></a></figure><figure id="02dc60d6-c936-4646-b509-52b3cb04d840" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><msub><mi>U</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>α</mi><mi>z</mi><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">LeakyReLU_a(z) = max(\alpha z, z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03148em;">ak</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="08ec2722-7e16-4c6f-bf8b-1d35a1fadf37" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span><span>﻿</span></span>가 이 함수가 ‘새는(leak)’ 정도를 결정한다.<ul id="a599b67f-5fdc-42df-aad2-93c192691539" class="bulleted-list"><li style="list-style-type:circle">새는 정도: <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">z &lt; 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span><span>﻿</span></span> 일 때 함수의 기울기. 이 기울기가 LeakyReLU를 절대 죽지 않게 만들어준다.</li></ul><ul id="4c4ffb32-e759-4c9f-ac90-d201d649a5dc" class="bulleted-list"><li style="list-style-type:circle">기울기가 클 때 조금 더 나은 성능을 냄 (<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.2</span></span></span></span></span><span>﻿</span></span>가 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\alpha=0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.01</span></span></span></span></span><span>﻿</span></span>보다 나은 성능)</li></ul></li></ul><ul id="e15dc3bb-d3b7-4657-852c-4691d9675a65" class="bulleted-list"><li style="list-style-type:disc">RReLU_Randomized Leaky ReLU<ul id="4680b4da-c1d1-4735-8de7-24d329c0dbeb" class="bulleted-list"><li style="list-style-type:circle">훈련하는 동안 주어진 범위에서 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span><span>﻿</span></span>를 랜덤으로 선택하고 테스트 시에는 ‘평균’을 사용</li></ul><ul id="410a0169-9da2-43bb-bf32-420ed777dd04" class="bulleted-list"><li style="list-style-type:circle">과대적합 위험을 줄이는 규제의 역할을 하는 것처럼 보임</li></ul></li></ul><ul id="9dfbd197-7e67-4aec-934e-cb117844cf8c" class="bulleted-list"><li style="list-style-type:disc">PReLU_parametric leaky ReLU<ul id="684d1284-a68c-42d4-9e24-a0fb58de06ed" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span><span>﻿</span></span>가 훈련하는 동안 학습됨</li></ul><ul id="580c90d9-b936-44ab-9df1-550abd538c49" class="bulleted-list"><li style="list-style-type:circle">하이퍼파라미터가 아니고, 다른 모델 파라미터와 같이 역전파에 의해 변경되는 것이다.</li></ul><ul id="c81005f5-fdbf-498b-8611-d46700e023bf" class="bulleted-list"><li style="list-style-type:circle">대규모 데이터셋에서만 ReLU보다 성능이 앞서고, 소규모 데이터셋에서는 과대적합될 위험이 있음.</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="24357692-7e2a-4e07-a608-16be28c93fa0" class="code"><code class="language-Python"># 활성화 함수를 별도의 층으로 모델에 추가할 수도 있음. (훈련과 예측 시 차이는 없음)
model = tf.keras.models.Sequential([
	[...] # 다른 층
	tf.keras.layers.Dense(50, kernel_initializer=&quot;he_normal&quot;), # 활성화 함수 없음
	tf.keras.layers.LeakyReLU(alpha=0.2), # 별도의 활성화 함수 층
	[...] # 다른 층
])</code></pre><h3 id="87ac4f49-ce03-4f4a-98ac-25b81ca6074b" class="">ELU와 SELU</h3><ul id="319e6f84-c7dc-485b-8f7f-7e742d8201e3" class="bulleted-list"><li style="list-style-type:disc">ReLU, LeakyReLU, PReLU의 도함수(기울기)는 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">z = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span><span>﻿</span></span>에서 갑자기 바뀐다.</li></ul><ul id="1eac2d85-69d4-42d7-860d-8d8f36fdf7d9" class="bulleted-list"><li style="list-style-type:disc">이러한 불연속성은 경사하강법을 최적점에서 진동하게 만들거나 수렴을 느리게 만들 수 있다.</li></ul><ul id="edbb8c4d-38af-4f90-8566-f96017a0cb0a" class="bulleted-list"><li style="list-style-type:disc">그래서 ELU, SELU와 같은 ReLU의 부드러운 변형이 나왔다.</li></ul><ul id="90a15ac2-7dc6-4076-b0b1-7e8266609d4a" class="bulleted-list"><li style="list-style-type:disc">ELU_exponential linear unit<figure id="7cb3dde1-56a2-447c-a3e4-8606d2216dbb" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%205.png"><img style="width:240px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%205.png"/></a></figure><ul id="8897fc47-f0c1-4b86-8046-d7c72b39f024" class="bulleted-list"><li style="list-style-type:circle">단점<ul id="ac12a4f0-d68b-490e-9e55-fd04a8da6d8f" class="bulleted-list"><li style="list-style-type:square">지수 함수를 사용하므로 ReLU나 그 변형들보다 계산이 느리다.</li></ul><ul id="c0279f59-e6fe-42bf-b1ce-c8f0c93f2397" class="bulleted-list"><li style="list-style-type:square">훈련하는 동안에는 수렴 속도가 빨라서 느린 계산이 상쇄될 수 있지만, 테스트 시에는 ELU를 사용한 네트워크가 ReLU를 사용한 네트워크보다 느릴 것.</li></ul></li></ul></li></ul><hr id="25f85472-1a62-4d32-9d59-0dd905deccab"/><figure id="821a21a5-33a8-492a-a53a-c12bc3b1ea04" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%206.png"><img style="width:480px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%206.png"/></a></figure><hr id="d8a5de8c-e273-4212-99ef-b67159bfca4d"/><ul id="80924ff0-0d99-476f-9394-a8f8aa6698b8" class="bulleted-list"><li style="list-style-type:disc">SELU_Scaled ELU<ul id="d364c572-bd18-47ef-8963-9346b464450a" class="bulleted-list"><li style="list-style-type:circle">스케일이 조정된 ELU 활성화 함수</li></ul><ul id="7e2e563d-8c8b-4df0-9387-f8b9f5b04299" class="bulleted-list"><li style="list-style-type:circle">MLP(특히 아주 깊은 네트워크)의 모든 은닉 층이 SELU를 사용한다면, 네트워크가 자기 정규화 된다.<ul id="bc488e3b-945d-4614-b316-349b90060075" class="bulleted-list"><li style="list-style-type:square">자기 정규화의 조건<ol type="1" id="42181339-3a31-4024-a957-f91d160e6392" class="numbered-list" start="1"><li>훈련하는 동안 각 층의 출력이 평균 0, 표준 편차 1 유지 (SELU는 유지하기 때문에 자기 정규화)<ul id="b7975a2e-fa7c-47bb-a824-02693b230d50" class="bulleted-list"><li style="list-style-type:disc">그레디언트 소실, 폭주 문제를 막아준다.</li></ul></li></ol><ol type="1" id="a1f81a36-2cff-47b6-98ee-c36da1c2def6" class="numbered-list" start="2"><li>모든 은닉 층의 가중치는 ‘르쿤 정규 분포 초기화’로 초기화되어야.</li></ol><ol type="1" id="542530eb-2aac-4b18-bb83-e1f56378c6e6" class="numbered-list" start="3"><li>일반적인 MLP에서만 보장됨. (스킵 연결과 같은 다루 구조에는 ELU보다 뛰어나지 않음)</li></ol><ol type="1" id="ffdc3d3d-8c89-4c9a-a106-4e2a7a3fe151" class="numbered-list" start="4"><li><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">l_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>, <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">l_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> 규제, 맥스-노름, 배치 정규화, 드롭아웃과 같은 규제를 사용할 수 없음.</li></ol></li></ul><ul id="1ed9e50c-a664-43ba-98e0-f817a01115d0" class="bulleted-list"><li style="list-style-type:square">이러한 제약이 크기 때문에 좋은 성질을 가짐에도 큰 관심을 얻지 못함.</li></ul><ul id="410d7654-2043-4e4c-914a-385366cfc0e9" class="bulleted-list"><li style="list-style-type:square">아래에서 설명할 GELU, Swish, Mish 활성화 함수가 대부분의 작업에서 일관되게 더 나은 성능을 발휘.</li></ul></li></ul></li></ul><h3 id="3b0ceeae-a1ea-4f5a-b193-b3de6ce252bf" class="">GELU, Swish, Mish</h3><figure id="cbb06cb8-0da6-46a4-a975-dbc5141ed0fa" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%207.png"><img style="width:432px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%207.png"/></a></figure><ul id="f5fbafca-1940-4eea-aa6b-f7e9efd3f88d" class="bulleted-list"><li style="list-style-type:disc">GELU: ReLU의 부드러운 변형<figure id="d711a5c7-71f3-4dc6-8a08-67105b89ee01" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>G</mi><mi>E</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>z</mi><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">GELU(z) = z\Phi(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">GE</span><span class="mord mathnormal" style="margin-right:0.10903em;">LU</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="cf02febe-ff33-490b-867a-1ac07632c944" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Phi(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span>: 표준 가우스 누적 분포 함수_cumulativ distribution function(CDF) = 평균이 0이고 분산이 1인 정규 분포에서 랜덤하게 샘플링한 값이 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span><span>﻿</span></span>보다 작을 확률</li></ul><ul id="1ae5f906-94a6-47f6-b59a-66ff9b40fb41" class="bulleted-list"><li style="list-style-type:circle">지금까지의 모든 활성화 함수는 convex monotonic(볼록한 단조) 함수이지만, GELU는 둘 다 아니다.<ul id="ec12dacf-01ac-4cef-93d6-212327aba47b" class="bulleted-list"><li style="list-style-type:square">왼쪽에서 직선으로 시작해서, 아래로 구부러지다가, -0.17에서 저점에 도달한다. 그리고 마지막에 상승하여 오른쪽 위로 직진한다.</li></ul><ul id="9e27828f-fe26-4cd7-9fb2-b54e998a95c7" class="bulleted-list"><li style="list-style-type:square">이런 복잡한 모양과 모든 위치에 곡률이 있기 때문에 복잡한 작업에서 이 함수가 잘 작동하는 것.</li></ul><ul id="031a404a-af85-4419-80bf-5c877935150f" class="bulleted-list"><li style="list-style-type:square">경사하강법이 복잡한 패턴을 학습하기 쉬워지는 것.</li></ul></li></ul><ul id="22fff0f6-1bb2-484f-80ef-98c68bc3d838" class="bulleted-list"><li style="list-style-type:circle">단점: 계산량이 많다. (성능 향상이 추가 비용을 정당화하기에 항상 충분하지는 않다.)</li></ul></li></ul><ul id="0004a813-da82-47a9-8dca-090ec51a1bf2" class="bulleted-list"><li style="list-style-type:disc">SiLU_sigmoid linear unit = Swish<figure id="32595856-ffe5-4dc4-b226-36a2c9264477" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mi>w</mi><mi>i</mi><mi>s</mi><msub><mi>h</mi><mi>β</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>z</mi><mi>σ</mi><mo stretchy="false">(</mo><mi>β</mi><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Swish_\beta(z) = z\sigma(\beta z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">Sw</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="c60981bc-a5ff-4647-b93c-9e134fabfbf0" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>1.702</mn></mrow><annotation encoding="application/x-tex">\beta = 1.702</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.702</span></span></span></span></span><span>﻿</span></span>를 사용하는 일반화된 Swish 함수 → GELU</li></ul><ul id="58019dc6-d6b1-4371-9efc-2d9487a946f9" class="bulleted-list"><li style="list-style-type:circle">다른 하이퍼파라미터처럼 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span><span>﻿</span></span>를 튜닝할 수 있음</li></ul><ul id="53b8416a-e03c-420a-b76d-aca42f6f0c5b" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span><span>﻿</span></span>를 훈련 가능한 파라미터로 만들고 경사 하강법으로 최적화할 수도 있음. (PReLU와 마찬가지로 과대적합 위험)</li></ul></li></ul><ul id="32837617-86b0-489d-bce0-802f7a558237" class="bulleted-list"><li style="list-style-type:disc">Mish<figure id="20b5781c-8df6-4052-8a8e-7530faef69a9" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>m</mi><mi>i</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>z</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>p</mi><mi>l</mi><mi>u</mi><mi>s</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">mish(z) = ztanh(softplus(z))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">mi</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span><span class="mopen">(</span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal" style="margin-right:0.01968em;">tpl</span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">))</span></span></span></span></span></div></figure><figure id="bbd076c1-71d5-4098-b2d7-8ffbcd8ab815" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>p</mi><mi>l</mi><mi>u</mi><mi>s</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">softplus(z) = log(1 + exp(z))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal" style="margin-right:0.01968em;">tpl</span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">))</span></span></span></span></span></div></figure><ul id="f2fbc44d-46db-4fa3-94da-ebaae692e6c4" class="bulleted-list"><li style="list-style-type:circle">볼록하지 않고 단조 함수가 아닌 ReLU의 변형</li></ul><ul id="8ec247b6-e0f7-4590-8e8e-1f6aa936d605" class="bulleted-list"><li style="list-style-type:circle">Swish보다 근소한 차이로 더 나은 성능 발휘</li></ul><ul id="cc1a2957-7212-4b5a-a956-e71c49af3415" class="bulleted-list"><li style="list-style-type:circle">z가 음수일 때 Swish와 거의 완벽하게 겹치고, z가 양수일 때 GELU와 거의 완벽하게 겹침</li></ul></li></ul><ul id="db836f29-3145-4b31-9cb5-52a723aed990" class="bulleted-list"><li style="list-style-type:disc">그래서 심층 신경망의 hidden layer에 어떤 활성화 함수를 사용해야하나? (keras)<ul id="5a7f47e8-d58c-4bb9-b852-47acecbbfce1" class="bulleted-list"><li style="list-style-type:circle">간단한 작업에는 ReLU가 기본값</li></ul><ul id="c44114e9-6cb8-4880-8e95-b3e6fb0d50bc" class="bulleted-list"><li style="list-style-type:circle"><del>복잡한 작업에는 Swish (keras 지원 X)</del><ul id="5383f4f7-8aeb-429b-9470-389337af9b80" class="bulleted-list"><li style="list-style-type:square"><del>가장 복잡한 작업에는 학습 가능한 </del><del><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span><span>﻿</span></span></del><del> 파라미터를 가진 Swish</del></li></ul></li></ul><ul id="0d5b6188-6433-486b-ac27-14b46d7f4bb8" class="bulleted-list"><li style="list-style-type:circle"><del>Mish는 조금 더 나은 결과를 얻을 수 있지만 계산량이 좀 더 많음 (keras 지원 X)</del></li></ul><ul id="63735042-4bad-4792-8ada-5ae098e2f11d" class="bulleted-list"><li style="list-style-type:circle">실행 속도가 중요한 경우 LeakyReLU</li></ul><ul id="5c7765de-38a7-4774-b4df-65363fa7ecda" class="bulleted-list"><li style="list-style-type:circle">심층 MLP의 경우 SELU (제약 조건 반드시 준수)</li></ul><ul id="9a273175-3d06-442e-bc74-38037ecc498e" class="bulleted-list"><li style="list-style-type:circle">시간, 컴퓨팅 성능에 여유 있으면 ‘교차 검증’을 사용하여 다른 활성화 함수도 평가.</li></ul></li></ul><p id="1eb59e69-4623-458f-b26b-2e1ab9a0f822" class="">
</p><h2 id="cd23d0dd-849e-4378-b8c3-c96d283424dc" class="">11.1.3 배치 정규화</h2><ul id="167a4aa9-3154-4913-89c3-abc7321a86bd" class="bulleted-list"><li style="list-style-type:disc">활성화 함수 + 초기화 방법 변경으로 훈련 ‘초기’ 단계에서 그레디언트 소실과 폭주 문제를 줄일 수 있지만, 훈련하는 동안 다시 발생하지 않으리란 보장은 없다.</li></ul><ul id="f638c9eb-c322-4da6-8696-f78cfc889008" class="bulleted-list"><li style="list-style-type:disc">배치 정규화_batch normalization(BN)<ul id="9a5a7245-3bd4-41d5-b4fa-8b14e7e0367b" class="bulleted-list"><li style="list-style-type:circle"><span style="border-bottom:0.05em solid">각 층에서</span> 활성화 함수를 통과하기 전이나 후에 연산을 하나 추가하는 것.</li></ul><ul id="533eba27-c0ff-4ff2-8bf9-6681f341b24a" class="bulleted-list"><li style="list-style-type:circle"><span style="border-bottom:0.05em solid">입력을 원점에 맞추고 정규화</span> → 각 층에서 두 개의 새로운 파라미터로 결괏값의 스케일을 조정하고 이동<ul id="f537c45e-46b8-47c1-be4c-151c18a9b143" class="bulleted-list"><li style="list-style-type:square">파라미터 1: 스케일 조정</li></ul><ul id="7dc09a61-b0c6-4fb5-9997-127ee60e7d96" class="bulleted-list"><li style="list-style-type:square">파라미터 2: 이동</li></ul></li></ul><ul id="727db5b7-115f-4201-a427-d9b079df0a71" class="bulleted-list"><li style="list-style-type:circle">신경망의 첫 번째 층으로 배치 정규화를 추가하면, 훈련 세트를 표준화할 필요가 없음<ul id="3318b735-a1ef-41cd-9ba1-2620d4a6311f" class="bulleted-list"><li style="list-style-type:square">StandardScaler, Normalization 클래스가 필요하지 않을 것.</li></ul><ul id="d7f16bfc-6f52-4441-92ed-cd4ef7d11f18" class="bulleted-list"><li style="list-style-type:square">StandardScaler는 훈련 데이터의 평균과 표준 편차를 사용하여 데이터를 정규화하는 반면, 배치 정규화는 훈련 데이터를 사용하여 훈련 중에 평균과 표준 편차를 추정하고, 이를 사용하여 입력을 정규화한다. 그래서 테스트(예측) 시에는 추가적인 데이터 전처리 과정이 필요하다.<figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="cb0ccad1-fcfd-4630-8caf-99cfbd1f421e"><div style="font-size:1.5em"><img class="icon" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/KakaoTalk_Photo_2024-05-04-19-46-47.png"/></div><div style="width:100%">    StandardScaler는 전체 데이터셋의 평균과 표준 편차를 계산한 후에 이를 사용하여 데이터를 정규화합니다. 즉, 훈련 과정에서 <span style="border-bottom:0.05em solid">전체 데이터셋을 보고 평균과 표준 편차를 계산</span>한 후, <span style="border-bottom:0.05em solid">이를 사용하여 모든 데이터를 정규화</span>합니다.<p id="c4899ec7-bc03-422e-8cd6-b15b772c4036" class="">    한편, 배치 정규화는 <span style="border-bottom:0.05em solid">각 층에서의 입력을 정규화</span>하는 과정을 말해요. 훈련 중에 <span style="border-bottom:0.05em solid">각 미니배치의 평균과 표준 편차를 계산</span>하고, <span style="border-bottom:0.05em solid">이를 사용하여 입력을 정규화</span>합니다. 이것은 네트워크의 안정성을 향상시키고 학습 속도를 높이는 데 도움이 됩니다. 따라서 StandardScaler와 배치 정규화는 데이터를 정규화하는 목적은 비슷하지만, 적용하는 방식과 시점이 다릅니다.</p></div></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="15ccdc56-2d93-4396-9358-007f7e1c2bff"><div style="font-size:1.5em"><img class="icon" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/KakaoTalk_Photo_2024-05-04-19-46-47%201.png"/></div><div style="width:100%">    &quot;배치&quot; 정규화라는 용어는 조금 혼란스러울 수 있어요. 배치 정규화는 일반적으로 미니배치(mini-batch) 데이터를 기반으로 하기 때문에 이러한 이름이 붙었습니다. 하지만 배치 정규화는 실제로 미니배치의 크기에만 의존하는 것이 아니라, 각 층의 입력에 대한 통계적 정규화를 수행하는 개념입니다.<p id="5a900133-a0e9-4b67-bcd2-39892ce555cf" class="">    이름에서 유래한 혼동을 피하기 위해, <span style="border-bottom:0.05em solid">&quot;층 정규화(Layer Normalization)&quot;</span>나 &quot;그룹 정규화(Group Normalization)&quot; 등과 같은 용어가 사용되기도 합니다. 이러한 용어들은 각각 층 단위로 정규화하거나 입력을 그룹 단위로 나누어 정규화하는 다른 정규화 방법을 나타냅니다.</p><p id="428cf384-3fb2-41e7-88fa-8bb1d710be0e" class="">    결국, &quot;배치&quot; 정규화는 주로 미니배치 데이터를 사용하여 입력을 정규화하지만, 이것이 정확한 용어인지는 조금 논란이 될 수 있습니다.</p></div></figure></li></ul></li></ul></li></ul><hr id="f07fea9f-f048-4e78-a4c6-922709ebd7ae"/><figure id="b74d5099-5ac9-4670-adb9-b9f3b122e1e8" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%208.png"><img style="width:192px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%208.png"/></a></figure><hr id="6a9f75c5-064c-4736-90c4-9099295eb1f3"/><ul id="57efb74f-2604-4c84-b022-e56489704ad2" class="bulleted-list"><li style="list-style-type:disc">테스트 시에는 어떻게 할까?<ul id="cd81e87e-d195-453a-a0cd-4717e467bdc5" class="bulleted-list"><li style="list-style-type:circle">샘플의 배치가 아니라 샘플 하나에 대한 예측을 만들어야 한다. 이러한 경우에는 입력의 평균과 표준 편자를 계산할 방법이 없다.</li></ul><ul id="354dbab2-c759-433a-bbfd-efd7e26560e1" class="bulleted-list"><li style="list-style-type:circle">훈련이 끝난 후, 전체 훈련 세트를 신경망에 통과시켜 배치 정규화 층의 각 입력에 대한 평균과 표준편차를 계산하면 된다.</li></ul><ul id="f7fc52a9-8411-4548-a1fb-eccd5fbb0f90" class="bulleted-list"><li style="list-style-type:circle">그러나 대부분 배치 정규화 구현은 층의 입력 평균과 표준 편차의 ‘이동 평균’을 사용해, 훈련하는 동안 최종 통계를 추정한다. 케라스의 BatchNormalization 층은 이를 자동으로 수행한다.</li></ul></li></ul><ul id="d342db37-9153-4217-a25f-4d6802ddc2f9" class="bulleted-list"><li style="list-style-type:disc">그레디언트 소실 문제 감소에 더불어 드롭아웃과 같은 <span style="border-bottom:0.05em solid">규제 역할</span>을 한다. 다른 규제 기법의 필요성을 줄여주는 것이다.<ul id="8cdbf78e-0a91-4e1e-b424-4988d45591a8" class="bulleted-list"><li style="list-style-type:circle">배치 정규화는 전체 데이터셋이 아니라 미니배치마다 평균과 표준 편차를 계산하므로 훈련 데이터에 일종의 ‘<span style="border-bottom:0.05em solid">잡음</span>’을 넣는다고 볼 수 있다.</li></ul><ul id="eb115951-0730-4f9b-af76-0ff7b7638890" class="bulleted-list"><li style="list-style-type:circle">이런 잡음은 훈련 세트에 과대적합되는 것을 방지하는 규제의 효과를 가지며, 미니배치의 크기가 클수록 효과는 줄어든다.</li></ul><ul id="58405c2c-25cc-4f31-bea0-370db43257df" class="bulleted-list"><li style="list-style-type:circle">하지만 부수 효과이기 때문에 드롭아웃과 함께 사용하는 것이 좋긴 하다.</li></ul></li></ul><ul id="5af33ebd-dbdd-4c4b-a7f8-835f8b1185e1" class="bulleted-list"><li style="list-style-type:disc">성능과 시간<ul id="90648d21-00b6-48ef-b2cd-6f206d0f8ee1" class="bulleted-list"><li style="list-style-type:circle">에포크마다 더 많은 시간이 걸리므로, 훈련이 오히려 느려질 수 있다.</li></ul><ul id="b7fb226d-a1bc-4785-b5f2-77bf605acd23" class="bulleted-list"><li style="list-style-type:circle">하지만, 수렴이 더 빨라지므로 보통 상쇄된다.</li></ul><ul id="6ffb96aa-7d2b-4be6-9745-af5770eb27b5" class="bulleted-list"><li style="list-style-type:circle">더 적은 에포크로 동일한 성능에 도달할 수 있어, 대부분 <span style="border-bottom:0.05em solid">실제로 걸리는 시간은 더 짧다.</span></li></ul></li></ul><h3 id="a3f7de3d-1b02-49ac-98ae-6882538042e4" class="">케라스로 배치 정규화 구현하기</h3><ul id="6e86f0ef-6ad7-4f94-aea6-8c0ec2359294" class="bulleted-list"><li style="list-style-type:disc">활성화 함수 이전에 배치 정규화 층 추가하기<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ce0784b7-4256-446b-8658-010cd0d68842" class="code"><code class="language-Python">model = tf.keras.Sequential([
    # 입력층
    tf.keras.layers.Input(shape=(28, 28)),

    # 첫 번째 은닉층
    tf.keras.layers.BatchNormalization(),  # 배치 정규화
    tf.keras.layers.Dense(300, activation=&#x27;relu&#x27;, kernel_initializer=&#x27;he_normal&#x27;),  # 300개의 뉴런, ReLU 활성화 함수, He 초기화

    # 두 번째 은닉층
    tf.keras.layers.BatchNormalization(),  # 배치 정규화
    tf.keras.layers.Dense(300, activation=&#x27;relu&#x27;, kernel_initializer=&#x27;he_normal&#x27;),  # 300개의 뉴런, ReLU 활성화 함수, He 초기화

    # 세 번째 은닉층
    tf.keras.layers.BatchNormalization(),  # 배치 정규화
    tf.keras.layers.Dense(300, activation=&#x27;relu&#x27;, kernel_initializer=&#x27;he_normal&#x27;),  # 300개의 뉴런, ReLU 활성화 함수, He 초기화

    # 출력층
    tf.keras.layers.Dense(10, activation=&#x27;softmax&#x27;)  # 10개의 뉴런, 소프트맥스 활성화 함수
])</code></pre></li></ul><ul id="9852c99a-945d-4eee-9c1b-b69167842f26" class="bulleted-list"><li style="list-style-type:disc">활성화 함수 이후에 배치 정규화 층 추가하기<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="694b2020-ebbc-489f-8ca1-06e231feea6d" class="code"><code class="language-Python">model = tf.keras.Sequential([
    # 입력층
    tf.keras.layers.Input(shape=(28, 28)),

    # 첫 번째 은닉층
    tf.keras.layers.Dense(300, kernel_initializer=&#x27;he_normal&#x27;),  # 300개의 뉴런, He 초기화
    tf.keras.layers.BatchNormalization(),  # 배치 정규화
    tf.keras.layers.Activation(&#x27;relu&#x27;),  # ReLU 활성화 함수

    # 두 번째 은닉층
    tf.keras.layers.Dense(300, kernel_initializer=&#x27;he_normal&#x27;),  # 300개의 뉴런, He 초기화
    tf.keras.layers.BatchNormalization(),  # 배치 정규화
    tf.keras.layers.Activation(&#x27;relu&#x27;),  # ReLU 활성화 함수

    # 세 번째 은닉층
    tf.keras.layers.Dense(300, kernel_initializer=&#x27;he_normal&#x27;),  # 300개의 뉴런, He 초기화
    tf.keras.layers.BatchNormalization(),  # 배치 정규화
    tf.keras.layers.Activation(&#x27;relu&#x27;),  # ReLU 활성화 함수

    # 출력층
    tf.keras.layers.Dense(10, activation=&#x27;softmax&#x27;)  # 10개의 뉴런, 소프트맥스 활성화 함수
])</code></pre></li></ul><ul id="c861c236-0187-4b81-9092-563922aaa037" class="bulleted-list"><li style="list-style-type:disc">BatchNormalization의 하이퍼파라미터<ul id="a0cc0d70-784d-43e0-bf9d-514451b6c884" class="bulleted-list"><li style="list-style-type:circle">momentum: 지수 이동 평균을 업데이트할 때 사용<ul id="3cfc49f0-b38e-4f32-963d-e9deab47b1d3" class="bulleted-list"><li style="list-style-type:square"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>v</mi><mo>^</mo></mover><mo>←</mo><mover accent="true"><mi>v</mi><mo>^</mo></mover><mo>×</mo><mi>m</mi><mi>o</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>u</mi><mi>m</mi><mo>+</mo><mi>v</mi><mo>×</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>m</mi><mi>o</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>u</mi><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{v} \leftarrow \hat{v}\times momentum + v \times (1-momentum)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span></li></ul></li></ul><ul id="40368390-19bd-4aea-9d93-eb13fc8135ed" class="bulleted-list"><li style="list-style-type:circle">axis: 정규화할 축을 결정 (기본값은 -1, 마지막 축을 정규화.)<ul id="90193a24-bd22-4d2d-8940-9cc87165738e" class="bulleted-list"><li style="list-style-type:square">입력 배치가 2D([샘플 개수, 특성 개수])<ul id="1b67ed53-9fe7-4dde-8628-e187791a15c7" class="bulleted-list"><li style="list-style-type:disc">각 입력 특성이 (배치에 있는 모든 샘플에 대해 계산한 평균과 표준편차를 기반으로) 정규화.</li></ul></li></ul><ul id="e2c4a136-36a2-4a9f-82d8-e262ac8dd393" class="bulleted-list"><li style="list-style-type:square">입력 배치가 3D([샘플 개수, 높이, 너비]): 배치 정규화 층을 Flatten 층 이전으로 옮길 경우.<ol type="1" id="73b1fc24-91f5-43ce-8607-a523ab344290" class="numbered-list" start="1"><li>배치 정규화 층이 28개의 평균과 28개의 표준 편차를 계산.</li></ol><ol type="1" id="1f9e3b30-dbbb-4c9c-b056-176a5ac5933d" class="numbered-list" start="2"><li>그 다음, 동일한 평균과 표준 편차를 사용하여 해당 열의 모든 픽셀을 정규화한다.</li></ol><ol type="1" id="95201401-76eb-4bc4-b971-e40d3285e105" class="numbered-list" start="3"><li>784개 픽셀을 독립적으로 다루고 싶다면 axis=[1, 2]로 지정해야.</li></ol></li></ul></li></ul></li></ul><ul id="75a0ece2-ab75-4b2a-bd96-7fb3ecbccc41" class="bulleted-list"><li style="list-style-type:disc">CNN의 모든 층 뒤에 배치 정규화가 있는 것으로 가정함. (매우 널리 사용해서, 종종 신경망 그림에서 빠져있음.)</li></ul><p id="706cd886-afc5-48d2-90fe-9d8e4e25c4a4" class="">
</p><h2 id="1900e87c-622a-419b-84d2-e34bebbd952f" class="">11.1.4 그레디언트 클리핑_gradient clipping</h2><ul id="8fb212ea-864d-456c-a699-8277f6ec83fa" class="bulleted-list"><li style="list-style-type:disc">역전파될 때 특정 임계값을 넘어서지 못하도록 그레디언트를 잘라내는 것.</li></ul><ul id="f278d456-2ff0-4984-8177-aa8462605154" class="bulleted-list"><li style="list-style-type:disc">일반적으로 배치 정규화를 사용하기 까다로운 순환 신경망에서 사용됨.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a0bb62da-2dbe-4ac6-b64a-a0ada78546ea" class="code"><code class="language-Python">optimizer = tf.keras.optimizers.SGD(clipvalue=1.0) # cliipnorm 매개변수 지정
model.compile([...], optimizer=optimizer)</code></pre><ul id="f58a643b-5323-45d9-ab3d-f327e101803a" class="bulleted-list"><li style="list-style-type:disc">위 코드는, 훈련되는 각 파라미터에 대한 ‘손실의 모든 편미분값’을 -1.0에서 1.0으로 잘라냄.<ul id="8ceb8d3b-d28e-41fe-9cb4-3a777dc8726c" class="bulleted-list"><li style="list-style-type:circle">원래 그레디언트 벡터가 [0.9, 100.0]이라면 대부분 2번째 축 방향을 향하는데,</li></ul><ul id="4a8310ca-d4ee-4928-a948-252d603f0479" class="bulleted-list"><li style="list-style-type:circle">위 코드를 기반으로 클리핑되면 [0.9, 1.0]이 되고, 거의 두 축 사이 대각선 방향을 향한다.</li></ul></li></ul><ul id="8d0e0d8a-0550-427b-9353-1462ed49134a" class="bulleted-list"><li style="list-style-type:disc">만약, 그레디언트 클리핑이 그레디언트 벡터의 방향을 바꾸지 못하게 하려면 clipvalue 대신 clipnorm을 지정하여 노름으로 클리핑해야 한다.</li></ul><p id="6fe0acef-b203-4b55-84e1-ee4ecda9f529" class="">
</p><h1 id="5c21bea2-10d0-4b23-804a-4f475bda9aca" class="">11.2 사전 훈련된 층 재사용하기</h1><ul id="aa1ca297-b639-4418-8ed3-e679d53cf81d" class="bulleted-list"><li style="list-style-type:disc">전이 학습_transfer learning: 훈련 속도를 높이고, 필요한 훈련 데이터도 크게 줄여줌.</li></ul><ul id="2503efc6-158d-4ced-929f-e0b8a0bee19f" class="bulleted-list"><li style="list-style-type:disc">만약 입력 이미지 크기가 다르다면, 원본 모델에 맞는 크기로 변경하는 ‘전처리’ 단계를 추가해야 한다.</li></ul><figure id="4711a3c8-5b29-46d3-b4c2-305a4922ae32" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%209.png"><img style="width:480px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%209.png"/></a></figure><ul id="018a4d77-24bd-4f48-a058-11feb12ec0fe" class="bulleted-list"><li style="list-style-type:disc">작업이 비슷할수록 (낮은 층부터 시작해서) 더 많은 층을 재사용하자. 아주 비슷한 작업이면 모든 은닉 층을 유지하고 출력 층만 교체하면 된다.<ul id="083a0d9f-639a-4a19-b81c-4dc780f3ebf6" class="bulleted-list"><li style="list-style-type:circle">원본 모델의 상위 은닉 층은 하위 은닉 층보다 덜 유용하기 때문.</li></ul></li></ul><ul id="1c1c0d34-0e53-424c-a431-76758dac7a22" class="bulleted-list"><li style="list-style-type:disc">재사용할 층 개수를 어떻게 선정?<ol type="1" id="4b9030a3-e1ec-45aa-903b-bde54f24a806" class="numbered-list" start="1"><li>재사용하는 층을 모두 동결 (가중치 고정)</li></ol><ol type="1" id="296411fd-a601-454e-ab1a-f271a4e68936" class="numbered-list" start="2"><li>모델 훈련 및 성능 평가</li></ol><ol type="1" id="1555572c-4e46-4b76-bd44-caf90269f72e" class="numbered-list" start="3"><li>맨 위에 있는 1-2개의 은닉층 동결을 해제하고 역전파를 통해 가중치를 조절하여 성능이 향상되는지 확인<ol type="a" id="8cf33c93-1f23-4175-b359-9d830a384a5a" class="numbered-list" start="1"><li>훈련 데이터가 많을수록 많은 층의 동결을 해제할 수 있음.</li></ol><ol type="a" id="c6f2fcf3-158b-418e-b30f-0bc9a0b2f697" class="numbered-list" start="2"><li>학습률을 줄이는게 좋음. (가중치를 세밀하게 튜닝하는 데 도움이 됨.)</li></ol></li></ol><ol type="1" id="dd32e053-ac91-4622-aa07-cdc1b4ec1895" class="numbered-list" start="4"><li>여전히 좋은 성능이 안나오고 훈련 데이터가 적다면, 상위 은닉층들을 제거하고 남은 은닉층을 다시 동결해보자.</li></ol><ol type="1" id="a6623bbd-0db4-4970-ad16-6ed1292588c6" class="numbered-list" start="5"><li>재사용할 은닉 층의 개수를 찾을 때까지 반복하기.</li></ol><ol type="1" id="8717f4da-3c42-4e61-9f3e-87b99e685905" class="numbered-list" start="6"><li>훈련 데이터가 아주 많다면 오히려 은닉 층을 추가하는 것도 방법.</li></ol></li></ul><h2 id="cbff207b-f2b7-44e2-9eb8-deddff5b2723" class="">11.2.1 케라스를 사용한 전이 학습</h2><ul id="52344698-408e-4a02-bdd1-a8aa247e3e77" class="bulleted-list"><li style="list-style-type:disc">출력 층만 제외하고 모든 층 재사용하여 model_B_on_A 만들기<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="7af021f0-a1d4-41ff-8197-78415c8b35c4" class="code"><code class="language-Python">model_A = tf.keras.models.load_model(&quot;my_model_A&quot;)
model_B_on_A = tf.keras.Sequential(model_a.layeres[:-1])
model_B_on_A.add(tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;))</code></pre></li></ul><ul id="6c7ed8c6-2598-4b94-8a9d-8d68972fde5d" class="bulleted-list"><li style="list-style-type:disc">model_B_on_A를 훈련할 때 model_A가 영향 받지 않게 하려면, model_A를 클론(clone)한 후 재사용하기<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="5cf8e059-fa92-4c17-b3e4-36320f4cc52d" class="code"><code class="language-Python">model_A_clone = tf.keras.models.clone_model(model_A) # 가중치 제외, 구조만 복제
model_A_clone.set_weights(model_A.get_weights())     # 가중치를 수동으로 복사해야. 안하면 랜덤 초기화됨.</code></pre></li></ul><ul id="2a8187bd-f75b-44fa-803a-ada61bbb0136" class="bulleted-list"><li style="list-style-type:disc">model_B_on_A 훈련 시, 새로운 층이 랜덤하게 초기화되어 있어 처음 몇 번의 epoch 동안 큰 오차가 발생할 수 있다. 이를 방지하기 위해 <span style="border-bottom:0.05em solid">처음 몇 번의 에포크 동안 재사용된 층을 동결</span>하고, 새로운 층에게 적절한 가중치를 학습할 시간을 주어야.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c0995abe-ddcf-48af-9fc7-4ddf8ba381b7" class="code"><code class="language-Python">for layer in model_B_on_A.layers[:-1]:
		layer.trainable = False # 처음 몇 번의 에포크 동안 재사용된 층을 동결

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
# 동결을 하거나 해제한 후, 반드시 모델 컴파일을 해야한다. (compile는 훈련될 가중치를 모으기 때문.)
model_B_on_A.compile(loss=&quot;binary_crossentropy&quot;, optimizer=optimizer,
											metrics=[&quot;accuracy&quot;])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
													 validation_data=(X_valid_B, y_valid_B))

#----------------------------------------------------------------

for layer in model_B_on_A.layers[:-1]:
		layer.trainable = True
		
optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
# 동결을 하거나 해제한 후, 반드시 모델 컴파일을 해야한다. (compile는 훈련될 가중치를 모으기 때문.)
model_B_on_A.compile(loss=&quot;binary_crossentropy&quot;, optimizer=optimizer,
											metrics=[&quot;accuracy&quot;])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
													 validation_data=(X_valid_B, y_valid_B))</code></pre></li></ul><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="7010ad15-5ddc-49a4-8a8a-6d5b8b291596"><div style="font-size:1.5em"><img class="icon" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/KakaoTalk_Photo_2024-05-04-19-46-47%201.png"/></div><div style="width:100%">    전이 학습(Transfer Learning)은 학습된 모델이나 그 모델의 일부를 새로운 작업에 적용하는 기술입니다. <span style="border-bottom:0.05em solid">전이 학습이 작은 fully connected network에서는 잘 작동하지 않는 이유</span>는 <span style="border-bottom:0.05em solid">주어진 작은 네트워크가 특정 작업에 특화되어 있기 때문</span>입니다. 작은 네트워크는 제한된 데이터로 학습되었으며, 이는 작은 데이터셋에 대해 더 <span style="border-bottom:0.05em solid">일반적인 패턴을 학습하기 어렵게 만듭니다.</span><p id="daceb9f9-b5bf-4515-b4e0-7e2ca6d7fcf4" class="">    Fully connected network는 특정 &#x27;패턴&#x27;을 학습하기 때문에 이러한 <span style="border-bottom:0.05em solid">패턴은 다른 작업에 유용하지 않을 수 있습니다.</span> 따라서 작은 fully connected network에서는 전이 학습이 잘 작동하지 않을 수 있습니다.</p><p id="96c31a69-cbe5-4a7e-8163-c3b7c7d0c9e5" class="">    그러나 CNN(Convolutional Neural Network)과 같은 모델에서는 전이 학습이 더 잘 작동할 수 있습니다. <span style="border-bottom:0.05em solid">CNN은 이미지와 같은 데이터에서 공간적인 특징을 감지하기 위해 설계</span>되었으며, <span style="border-bottom:0.05em solid">이러한 특징은 다양한 작업에서 유용</span>할 수 있습니다. <span style="border-bottom:0.05em solid">특히 아래쪽 층에서는 이미지의 일반적인 특성을 감지하는 경향</span>이 있기 때문에, 이러한 층에서 전이 학습을 적용하는 것이 효과적일 수 있습니다.</p><p id="359eb2c2-5ce7-40e8-a294-153c275984df" class="">    따라서 전이 학습은 작은 fully connected network보다 CNN과 같은 모델에서 보다 일반적인 특성을 학습한 경우에 더 잘 작동할 수 있습니다.</p></div></figure><h2 id="f650c634-75fd-4568-b79f-d9a70127efe7" class="">11.2.2 비지도 사전 훈련_unsupervised pretraining</h2><ul id="eb2ae69d-415a-4055-8ccf-aec704304567" class="bulleted-list"><li style="list-style-type:disc">레이블된 훈련 데이터가 많지 않은 경우이고, 비슷한 작업에 대해 훈련된 모델을 찾을 수 없을 때.</li></ul><ul id="86adfaff-6182-42dc-83c0-b8d9fa91288b" class="bulleted-list"><li style="list-style-type:disc">레이블이 없는 훈련 샘플을 모으는 것은 비용이 적게 들지만, 레이블을 부여하는 것이 비싸다.</li></ul><ul id="37dcdd71-87f5-420c-9044-2190b5a20858" class="bulleted-list"><li style="list-style-type:disc">비지도 학습 과정<ol type="1" id="5d763c3d-6417-430d-b116-0773258a1375" class="numbered-list" start="1"><li>레이블되지 않은 훈련 데이터를 많이 모을 수 있다면, 이를 사용하여 오토인코더(autoencoder)나 GAN과 같은 비지도 학습 모델을 훈련할 수 있다.<figure id="0ab39cbc-7d02-4b8c-bfff-effca0d291c7" class="link-to-page"><a href="https://www.notion.so/Autoencoder-0ab39cbc7d024b8cbfffeffca0d291c7?pvs=21">Autoencoder</a></figure><figure id="815979b6-22e7-49db-8b8a-cea665b32064" class="link-to-page"><a href="https://www.notion.so/GAN-815979b622e749db8b8acea665b32064?pvs=21">GAN</a></figure></li></ol><ol type="1" id="c5684f89-b185-4fd6-9502-395c1e0edaa1" class="numbered-list" start="2"><li>그 다음, autoencoder나 GAN 판별자의 하위 층을 재사용하고 그 위에 새로운 작업에 맞는 출력 층을 추가할 수 있다,</li></ol><ol type="1" id="e085b20b-0f87-41bc-9a65-4f0e2dee62b7" class="numbered-list" start="3"><li>그리고 지도학습으로(레이블된 훈련 샘플로) 최종 네트워크를 세밀하게 튜닝한다.</li></ol></li></ul><figure id="9cf6aeca-1f16-43f8-b0c2-cda7b770b507" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2010.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2010.png"/></a></figure><h2 id="d8d8b09e-7979-4272-b409-9f86fd33599e" class="">11.2.3 보조 작업에서 사전 훈련</h2><ul id="c17a7fcc-1007-4a92-86c3-3edfd279da8b" class="bulleted-list"><li style="list-style-type:disc">훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에서 첫 번째 신경망을 훈련하고, 이 신경망의 하위 층을 실제 작업을 위해 사용한다.<ul id="7ccf5e8f-81f9-4413-8aa7-75172a92a0fb" class="bulleted-list"><li style="list-style-type:circle">ex1. 랜덤으로 많은 인물의 이미지를 수집해서 두 개의 다른 이미지가 같은 사람의 것인지 감지하는 첫 번째 신경망 훈련 가능.</li></ul><ul id="dc9b1d9e-54a2-494f-b526-d53d3d0eea29" class="bulleted-list"><li style="list-style-type:circle">ex2. 말뭉치(corpus) 다운로드 후 이 데이터에서 레이블된 데이터를 자동으로 생성. 일부 단어를  랜덤하게 지우고 누락된 단어를 예측하는 모델 훈련 가능</li></ul></li></ul><ul id="440ee019-b334-4b20-974c-86089df89070" class="bulleted-list"><li style="list-style-type:disc">자기 지도 학습_self-supervised learning: 데이터에서 스스로 레이블을 생성하고 지도 학습 기법으로 레이블된 데이터셋에서 모델을 훈련하는 방법</li></ul><h1 id="25c9e05c-bcf4-4536-afde-1e467a567338" class="">11.3 고속 옵티마이저</h1><ul id="05eb0ad4-7d81-4da3-ba61-f9c3018ccc92" class="bulleted-list"><li style="list-style-type:disc">이제까지 알아본 훈련 속도 높이는 방법<ol type="1" id="4fb1a8b4-583c-4a41-8817-d9db77c92583" class="numbered-list" start="1"><li>연결 가중치에 좋은 ‘초기화 전략’ 사용하기</li></ol><ol type="1" id="e9700f5d-76ae-4955-bccc-ae82955f771d" class="numbered-list" start="2"><li>좋은 ‘활성화 함수’ 사용하기</li></ol><ol type="1" id="b18eb1fc-ac0a-4033-9598-6587aa6f0bf6" class="numbered-list" start="3"><li>배치 정규화</li></ol><ol type="1" id="979ed3db-e03e-4e4a-bed4-5a16da1af35b" class="numbered-list" start="4"><li>(보조 작업 or 비지도 학습으로 만들 수 있는) pretrained network의 일부 재사용하기</li></ol></li></ul><ul id="6c46c6f8-32e1-4270-9a69-c69cd1c09ce3" class="bulleted-list"><li style="list-style-type:disc">또 다른 훈련 속도 높이는 방법 → gradient opimizer 대신 다른 opimizer<ol type="1" id="e9f43473-42c2-4eb8-8462-d2b5e608936f" class="numbered-list" start="1"><li>Momentum optimization</li></ol><ol type="1" id="c1c3054f-cc09-44c7-9243-1289d40c1b06" class="numbered-list" start="2"><li>네스테로프 가속 경사</li></ol><ol type="1" id="c689825b-9bb7-4003-b445-f2a88eb5aeea" class="numbered-list" start="3"><li>AdaGrad</li></ol><ol type="1" id="d5308693-5725-4c0e-99cb-fa1e834be726" class="numbered-list" start="4"><li>RMSProp</li></ol><ol type="1" id="4a0e9ce6-9f10-4db9-820a-356975d421b9" class="numbered-list" start="5"><li>Adam</li></ol><ol type="1" id="b25a962a-934c-41ab-81e6-9d055d8ff6a5" class="numbered-list" start="6"><li>Adam의 변형</li></ol></li></ul><h2 id="574efcdd-3de2-4dc4-ae09-1fccba9e1e60" class="">11.3.1 모멘텀 최적화</h2><figure id="ce3946ef-3ad6-4650-bebc-8c129de3e0c9" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>m</mi><mo>←</mo><mi>β</mi><mi>m</mi><mo>−</mo><mi>η</mi><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">m \leftarrow \beta m - \eta \nabla_\theta J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="56699234-ad93-4e5b-9fb8-83f3fae5aedc" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta + m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span></div></figure><ul id="d9c375da-81f2-43cc-b43e-2bdfbd5aba55" class="bulleted-list"><li style="list-style-type:disc">모멘텀 최적화 원리: 볼링공이 terminal velocity(종단 속도)에 도달할 때까지는 빠르게 가속됨.<ul id="38960692-79aa-4cb9-b77d-deee3bcee7aa" class="bulleted-list"><li style="list-style-type:circle">이전 그레디언트가 얼마였는지 반영함.</li></ul><ul id="075c5361-f333-41e7-819f-56b3fa45f738" class="bulleted-list"><li style="list-style-type:circle">그레디언트를 ‘속도’가 아니라 ‘가속도’로 사용하는 것.</li></ul><ul id="f9b37d44-0556-42f7-8980-fa3527f139ce" class="bulleted-list"><li style="list-style-type:circle">모멘텀이 너무 커지는 것을 막기 위해 하이퍼파라미터 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span><span>﻿</span></span>(모멘텀)를 사용함.<ul id="c367c271-c14c-4865-8603-b96d5aba06db" class="bulleted-list"><li style="list-style-type:square"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span><span>﻿</span></span>가 커질수록 (저항이 없어질수록) 최적점 주위에서 수렴을 위한 진동이 심해진다.</li></ul><ul id="3d631716-95cb-44b0-9ed2-146319c3bfc0" class="bulleted-list"><li style="list-style-type:square">모멘텀 때문에 옵티마이저가 최적값에 안정되기 전까지 왔다갔다 할 수 있기 때문에, 시스템에 마찰이 조금 있는 것이 이러한 진동을 없애고 빠르게 수렴되는 데 도움이 된다.</li></ul></li></ul><ul id="7f25c06f-e467-4c43-92ae-15586174637c" class="bulleted-list"><li style="list-style-type:circle">배치 정규화를 사용하지 않아 비용함수 그래프가 한쪽으로 길쭉한 모양인 경우, 표준 경사하강법은 좁고 긴 골짜기에서 오래 걸린다. 반면, 모멘텀 최적화는 골짜기를 따라 최적점에 도달할 때까지 점점 더 빨리 내려가기 때문에 큰 도움이 된다.</li></ul><ul id="5916f79a-1fd1-4fca-8643-3e89d7611acb" class="bulleted-list"><li style="list-style-type:circle">local minimum을 건너뛰는 데에도 도움이 됨.</li></ul><ul id="d8db504d-1ccf-4511-8251-6b5f3614f5ea" class="bulleted-list"><li style="list-style-type:circle">단점: 튜닝할 하이퍼파라미터(momentum)가 하나 더 늘어난다.</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e6adc09e-b714-4fb6-81a5-4cd015b1a62d" class="code"><code class="language-Python">optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
# 보통 모멘텀 0.9에서 잘 작동하며, 경사하강법보다 거의 항상 더 빠르다.</code></pre><hr id="44faf97a-5eb2-4047-afec-6b44167096b2"/><ul id="fd960e67-b5b6-47f8-9558-cb69e4496cb3" class="bulleted-list"><li style="list-style-type:disc">표준 경사하강법: 경사가 완만할 때는 작은 스텝, 가파를 때는 큰 스텝. 하지만 ‘속도’가 높아지지는 않음.<ul id="f7ff7028-526c-4019-91b3-12252ffd7012" class="bulleted-list"><li style="list-style-type:circle">결과적으로 모멘텀 최적화보다 최저점에 도달하는데 훨씬 느림.</li></ul><ul id="b69cd73e-7b6e-45ba-a874-1ee87706b3d2" class="bulleted-list"><li style="list-style-type:circle"><span style="border-bottom:0.05em solid">가중치를 갱신할 때, 이전의 그레디언트가 얼마였는지 고려하지 않음.</span></li></ul></li></ul><figure id="99a3f674-a041-48a6-a7d1-3288af51163b" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>−</mo><mi>η</mi><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta - \eta \nabla_\theta J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></div></figure><h2 id="1c977735-1be6-46c2-a599-21cbb842fbb4" class="">11.3.2 네스테로프 가속 경사</h2><h2 id="30363aa8-43c2-43d8-a641-07d8ef2435bc" class="">11.3.3 AdaGrad</h2><ul id="90d5db2d-f521-40bb-bd47-1a45ec7337a6" class="bulleted-list"><li style="list-style-type:disc">한쪽이 길쭉한 그릇 문제에서, 경사하강법보다 빨리 (처음부터) 전역 최적점 쪽으로 방향을 잡으면 좋지 않을까?</li></ul><ul id="f920baf8-3965-474f-8f79-1849c6f219c8" class="bulleted-list"><li style="list-style-type:disc">AdaGrad는 가장 가파른 차원을 따라 그레디언트 벡터의 스케일을 감소시켜 이 문제를 해결.</li></ul><hr id="781103f1-719a-4dc3-a671-6911448f715c"/><ol type="1" id="fbcf73d0-826f-4d7f-a21e-1964531f1511" class="numbered-list" start="1"><li>그레디언트의 제곱을 벡터 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span><span>﻿</span></span>에 누적한다. (<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span></span><span>﻿</span></span>는 원소별 곱셈)</li></ol><figure id="e2c3c3d3-2768-4dc0-adce-e9702b689cf0" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mo>←</mo><mi>s</mi><mo>+</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>⊗</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s \leftarrow s + \nabla_\theta J(\theta) \otimes \nabla_\theta J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></div></figure><ol type="1" id="6b6c1d65-ebfc-43ec-aa33-833a452128e0" class="numbered-list" start="2"><li>경사하강법과 같은데, 그레디언트 벡터를 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mrow><mi>s</mi><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{s + \epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.205em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.835em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.795em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.205em;"><span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> 으로 나누어 스케일을 조정함. (<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊘</mo></mrow><annotation encoding="application/x-tex">\oslash</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊘</span></span></span></span></span><span>﻿</span></span>는 원소별 나눗셈, <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></span><span>﻿</span></span>은 일반적으로 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>10</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>)</li></ol><figure id="38035640-0966-4955-b946-cae274d531c5" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>−</mo><mi>η</mi><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>⊘</mo><msqrt><mrow><mi>s</mi><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta - \eta \nabla_\theta J(\theta) \oslash \sqrt{s + \epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊘</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1561em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8839em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.8439em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1561em;"><span></span></span></span></span></span></span></span></span></span></div></figure><hr id="3fd5bcfe-3782-410f-bc19-408c8a323b8e"/><figure id="77b8734c-a920-40c7-bb78-7f0937e402b0" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2011.png"><img style="width:528px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2011.png"/></a></figure><ul id="ae5aa20c-7c75-4c57-8a4a-d3d5b54fe6f8" class="bulleted-list"><li style="list-style-type:disc">학습률을 감소시키지만, (경사가 완만한 차원보다) 가파른 차원에 대해 더 빠르게 감소한다. = 적응적 학습률(adaptive learning curve)<ul id="583e630a-5449-40b4-88a0-c09710ebe249" class="bulleted-list"><li style="list-style-type:circle">장점: 학습률 하이퍼파라미터 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span></span><span>﻿</span></span>(learning rate)를 덜 튜닝해도 된다.</li></ul></li></ul><ul id="ec6b630d-e3c2-468b-8ed4-005b77231cd4" class="bulleted-list"><li style="list-style-type:disc">단점: 학습률이 너무 감소하여 global minimum에 도착하기 전에 (너무 일찍) 최적화 알고리즘이 멈춘다.<ul id="bac6c07d-55dc-402a-8b16-2cc833f580ec" class="bulleted-list"><li style="list-style-type:circle">선형 회귀 같은 간단한 작업에만 효과적이고, 심층 신경망에는 사용하지 말아야.</li></ul></li></ul><hr id="443c5226-c9da-49e1-83d0-9158319d1c35"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="08ef9804-a303-4f1d-99b0-6d24d1f22f93" class="code"><code class="language-Python">optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)</code></pre><h2 id="c0e6c56c-883b-418e-9df5-abb8678cff72" class="">11.3.4 RMSProp (AdaGrad의 변형)</h2><ul id="aee4f6ec-434a-4e2c-88e0-8f3b1bb63816" class="bulleted-list"><li style="list-style-type:disc">AdaGrad는 너무 빨리 느려져서 global minimum에 수렴하지 못하는 위험이 있음.<ul id="a3e5d13c-40d6-4c13-8506-77bd047452c0" class="bulleted-list"><li style="list-style-type:circle">AdaGrad의 발전된 버전이므로 적응적 학습률 알고리즘이기 때문에, <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span></span><span>﻿</span></span>를 튜닝할 필요가 적다.</li></ul></li></ul><ul id="51f242d8-4438-4d9f-a254-b936399bc682" class="bulleted-list"><li style="list-style-type:disc">RMSProp은 훈련 시작부터의 모든 그레디언트가 아닌, 가장 최근 반복에서 비롯된 그레디언트만 누적함으로써 해결함.</li></ul><hr id="f868b549-cdb4-4b29-97fb-8ed2ad2bdc2e"/><p id="ea35328f-3caf-464b-899f-dfda397bf15d" class="">(알고리즘 첫 번째에 지수 감소를 사용. (보통 감쇠율 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span></span></span></span></span><span>﻿</span></span>를 0.9로 설정함.))</p><figure id="ff1ea5b3-40d7-419a-89be-5fe232864a06" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mo>←</mo><mi>ρ</mi><mi>s</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ρ</mi><mo stretchy="false">)</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>⊗</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s \leftarrow \rho s + (1-\rho)\nabla_\theta J(\theta) \otimes \nabla_\theta J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ρ</span><span class="mclose">)</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="891fecd9-7495-4e57-ad73-ef361389914f" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>−</mo><mi>η</mi><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>⊘</mo><msqrt><mrow><mi>s</mi><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta - \eta \nabla_\theta J(\theta) \oslash \sqrt{s + \epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊘</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1561em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8839em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.8439em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1561em;"><span></span></span></span></span></span></span></span></span></span></div></figure><hr id="024ce92f-df5f-4723-80f4-5a633b5d05f9"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e97bd8b6-018e-4af3-a1f8-459ac0ad13d3" class="code"><code class="language-Python">optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
# 튜닝할 하이퍼파라미터가 하나 더 생겼지만,
# 기본값이 잘 동작하는 경우가 많으므로 하이퍼파라미터 rho를 튜닝할 필요는 없다.</code></pre><hr id="60145111-beca-4ef0-9161-5368bb6c243d"/><ul id="0d3e3794-c255-4bba-a348-d81a58645b67" class="bulleted-list"><li style="list-style-type:disc">AdaGrad &lt; RMSProp &lt; Adam</li></ul><h2 id="bafd4bfd-76ea-42b0-b9d0-7b5d7e78d7c2" class="">11.3.5 Adam (모멘텀 최적화 + RMSProp)</h2><ul id="ddb50ea6-c561-4aab-8940-902aecb54c7c" class="bulleted-list"><li style="list-style-type:disc">Adam = Adaptive moment estimation (적응형 모멘트 추정)<ul id="9413ad03-3e9a-4d67-9ffd-ad74a56c6fb3" class="bulleted-list"><li style="list-style-type:circle">모멘텀 최적화 + RMSProp<ul id="f4136302-14db-4adf-877a-4600d2c6f3a2" class="bulleted-list"><li style="list-style-type:square">모멘텀 최적화: 지난 그레디언트의 지수 감소 평균(exponential decaying avg)를 따름.</li></ul><ul id="36e6dee8-5591-4998-ae6a-7392cc2e4441" class="bulleted-list"><li style="list-style-type:square">RMSProp: 지난 그레디언트 제곱의 지수 감소된 평균을 따름.</li></ul></li></ul><ul id="5cc360b5-5491-4a70-8162-2769a61221b5" class="bulleted-list"><li style="list-style-type:circle">그레디언트 평균과 (평균이 0이 아닌) 분산에 대한 예측임.<ul id="3af33268-f501-4e94-88ea-b9c92bfb898d" class="bulleted-list"><li style="list-style-type:square">first moment: 평균</li></ul><ul id="c3bb847f-2b68-46c0-9dc9-76a8542b80dd" class="bulleted-list"><li style="list-style-type:square">second momnet: 분산</li></ul></li></ul><ul id="69898619-5424-427d-bb7f-b1b824c40f67" class="bulleted-list"><li style="list-style-type:circle">적응형 학습률 알고리즘이기 때문에 하이퍼파라미터 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span></span><span>﻿</span></span>를 튜닝할 필요가 적다.</li></ul></li></ul><hr id="53495bfc-c10f-4672-9510-e1bcc2739b01"/><figure id="07ac93d7-e3c1-4e1b-ab9a-5ee42e0c8427" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2012.png"><img style="width:288px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2012.png"/></a></figure><ul id="f4bdf27b-403b-4308-80c1-576378260997" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\beta_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>: 모멘텀 최적화 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span><span>﻿</span></span></li></ul><ul id="e4a82b2f-af26-49be-840c-14a64811e2a2" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\beta_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>: RMSProp의 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span></span></span></span></span><span>﻿</span></span> (감쇠율)</li></ul><hr id="e64d8d04-de58-4f15-8776-e88a981c7fe1"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d5c719bc-244e-4c79-8eb1-268d22665eb7" class="code"><code class="language-Python">optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,
																     beta_2=0.999)
# 보통 0.9, 0.999로 초기화하는 경우가 많음 (기본값)
# 그래서 경사하강법보다 Adam이 사용하기 더 쉽다.</code></pre><h2 id="35b6bed1-f01b-43c2-964a-0719d460654e" class="">11.3.6 AdaMax (Adam의 변형)</h2><h2 id="6b439a2a-9295-40f5-a36f-b4907d74990c" class="">11.3.7 Nadam (Adam의 변형)</h2><ul id="c744bf6e-f4b8-4994-91a7-13fe78660712" class="bulleted-list"><li style="list-style-type:disc">Adam + 네스테로프</li></ul><h2 id="334c409e-540e-486d-907c-ad512bd48c97" class="">11.3.8 AdamW (Adam의 변형)</h2><figure id="55e42e90-e4ad-48cb-b896-0c7e0bc3e0d4" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2013.png"><img style="width:528px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2013.png"/></a></figure><figure id="4737ef2f-d06a-455a-a306-62b6ec935738" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2014.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2014.png"/></a></figure><h2 id="68ed850b-3b5b-4eef-8f52-9c17bbb949e1" class="">11.3.9 학습률 스케줄링</h2><figure id="a5b35926-2d0c-472e-972c-d16d65d127d5" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2015.png"><img style="width:528px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2015.png"/></a></figure><ul id="e5371e05-6c1e-476c-bdd9-3a82fee6db4e" class="bulleted-list"><li style="list-style-type:disc">보라색: 처음에는 빠르게 진행, 최적점 근처에서 요동이 심해져 수렴하지 못함.</li></ul><hr id="6bf10fcc-944f-4fa0-a14b-d5af71f09d31"/><ul id="ffea32b5-dbfe-4bd6-ac04-36cc7a5a96d2" class="bulleted-list"><li style="list-style-type:disc">매우 작은 값에서 매우 큰 값까지 지수적으로 학습률을 증가시키면서 어떤 학습률이 좋은지 알아보면 된다.</li></ul><ul id="166450f4-4a76-4cfb-8ae0-0dbb4724e918" class="bulleted-list"><li style="list-style-type:disc">그러나, 일정한 학습률보다… 큰 학습률로 시작하고 학습 속도가 느려질 때 학습률을 낮추면 더 빨리 좋은 솔루션을 발견할 수 있다. = 학습 스케줄</li></ul><h3 id="0db977ac-1a5a-4ffe-9b7c-003ed758c94b" class="">1) 거듭제곱 기반 스케줄링_power scheduling</h3><figure id="013aba6e-5aaa-4f7a-8338-e74a88c941d5" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2016.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2016.png"/></a></figure><figure id="f2bcc838-e1ee-407e-a141-1ef5365626a7" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2017.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2017.png"/></a></figure><h3 id="da8da119-9da3-45d3-ba65-0c805a9a4ed4" class="">2) 지수 기반 스케줄링_exponential scheduling</h3><figure id="d03daf92-cbbd-45e7-9c75-5d0fd4c5f439" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2018.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2018.png"/></a></figure><figure id="d949283a-b7ce-480d-9298-f57c9cbd5c28" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2019.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2019.png"/></a></figure><ul id="6a747efd-f207-4c32-b74a-491041c3d66c" class="bulleted-list"><li style="list-style-type:disc"><code>tf.keras.callbacks.LearningRateScheduler</code></li></ul><figure id="a6366f41-6d52-45bb-9923-ff385b6659bb" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2020.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2020.png"/></a></figure><ul id="4dcd8b58-4891-46e4-a73b-8daf3617bf6a" class="bulleted-list"><li style="list-style-type:disc"><code>tf.keras.optimizers.schedules</code> 에 있는 스케줄 중 하나를 사용해 학습률을 정의하고 optimizer에 전달하기.<ul id="3fd87724-778c-42d3-962b-52817908df6f" class="bulleted-list"><li style="list-style-type:circle">epoch가 아니라, 매 스텝마다 학습률을 업데이트한다.</li></ul></li></ul><figure id="67bc555d-864e-4ae3-b9ea-34d7153e57af" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2021.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2021.png"/></a><figcaption>exponential_decay_fn()과 동일한 지수 기반 스케줄링을 구현하는 방법임.</figcaption></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="247b594d-5593-4e3e-9ec6-189335f964b3"><div style="font-size:1.5em"><img class="icon" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/KakaoTalk_Photo_2024-05-04-19-46-47%201.png"/></div><div style="width:100%">콜백(callback)은 TensorFlow와 같은 딥러닝 프레임워크에서 모델 학습 과정 중에 특정 이벤트가 발생했을 때 실행되는 사용자 지정 함수입니다. 콜백은 모델 학습을 모니터링하고, 중지하거나 수정하는 데 사용됩니다. 주요 이벤트로는 에포크(epoch)의 시작 또는 종료, 배치(batch)의 시작 또는 종료, 학습률의 조정 등이 있습니다.<p id="04b51e86-f078-4542-8e46-0d8fe39f98d9" class="">콜백은 다음과 같은 목적으로 사용될 수 있습니다:</p><ol type="1" id="f54b3694-b042-4424-abd5-ca2df4463593" class="numbered-list" start="1"><li><strong>모델 체크포인트 저장</strong>: 특정 조건이 충족되었을 때 모델의 가중치를 저장합니다. 이렇게 함으로써 학습 중간에 모델의 상태를 저장하고, 이후에 모델을 재사용하거나 학습을 재개할 수 있습니다.</li></ol><ol type="1" id="3708f471-bf82-43d7-8b0b-ef27f66407fe" class="numbered-list" start="2"><li><strong>조기 종료(early stopping)</strong>: 검증 손실이 더 이상 개선되지 않을 때 학습을 중지합니다. 이렇게 함으로써 과적합을 방지하고 학습 속도를 높일 수 있습니다.</li></ol><ol type="1" id="f312dcab-619e-420f-b97f-23467acac1d7" class="numbered-list" start="3"><li><strong>학습률 조정</strong>: 학습률을 동적으로 조정하여 학습 과정을 최적화합니다. 학습률을 점진적으로 감소시키거나 증가시키는 등의 전략을 사용할 수 있습니다.</li></ol><ol type="1" id="35a4c93a-aeaa-4d02-9b9c-7f8ec2d1ce0a" class="numbered-list" start="4"><li><strong>로그 기록</strong>: 학습 중에 발생하는 다양한 지표를 기록하고, 그래프나 시각화 도구를 사용하여 학습 과정을 모니터링합니다.</li></ol><ol type="1" id="4cf21afc-14f2-431e-a078-e31e9243154c" class="numbered-list" start="5"><li><strong>사용자 정의 동작</strong>: 사용자가 정의한 특정 동작을 수행합니다. 예를 들어, 특정 지표가 특정 임계값을 초과하거나 미만할 때 알림을 보내는 등의 동작을 수행할 수 있습니다.</li></ol><p id="3d5309b0-0ffc-41ac-9961-bcd587a854ab" class="">콜백은 TensorFlow의 <code><strong>tf.keras.callbacks</strong></code> 모듈에서 제공되며, 사용자 정의 콜백을 작성하여 필요한 동작을 구현할 수도 있습니다. 종종 콜백은 모델 학습 과정을 효율적으로 관리하고, 모델의 성능을 향상시키는 데 중요한 역할을 합니다.</p></div></figure><h3 id="323219fa-f8b8-4c23-a278-0f119bb848fe" class="">3) 구간별 고정 스케줄링_piecewise constant scheduling</h3><figure id="67e82cde-6408-4ec3-bcce-cd9278d22c2c" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2022.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2022.png"/></a></figure><figure id="2555fcb6-3c46-4d9f-9e76-9c5a6afef64a" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2023.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2023.png"/></a></figure><ul id="ee7fafa0-c166-40ce-80f2-6853755cb1a0" class="bulleted-list"><li style="list-style-type:disc">지수 기반 스케줄링과 마찬가지로, piecewise_constant_fn 함수로 LearningRateScheduler 콜백을 만들어 fit() 메서드에 전달한다. </li></ul><h3 id="77414319-6bff-4a4f-a77b-631fc524042e" class="">4) 성능 기반 스케줄링_performance scheduling</h3><figure id="9a0f3681-beef-4cc8-8d62-7040d1389e33" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2024.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2024.png"/></a></figure><ul id="7521d1f6-1965-41ce-a0d5-78af6938a7be" class="bulleted-list"><li style="list-style-type:disc"><code>tf.keras.callbacks.ReduceLROnPlateau</code></li></ul><figure id="479db508-ee4b-483c-9374-5d30eafdc2eb" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2025.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2025.png"/></a><figcaption>검증 손실(validation loss)이 5번의 연속적인 epoch 동안 향상되지 않을 때마다 학습률에 0.5를 곱한다.</figcaption></figure><h3 id="cbfa3db0-2d37-4002-b2b1-4fbef7fc4d80" class="">5) 1사이클 스케줄링</h3><ul id="7ca06fb8-d9b2-4f90-ac95-9f1949d2577c" class="bulleted-list"><li style="list-style-type:disc">학습률</li></ul><figure id="dd2257fa-8a38-4422-a5a6-2e661f5fb0cc" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2026.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2026.png"/></a></figure><figure id="43f13682-fe36-4133-a97e-615b0ff43e07" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2027.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2027.png"/></a></figure><ul id="40bd5456-6c43-4638-b6c5-a81a78d398f6" class="bulleted-list"><li style="list-style-type:disc">모멘텀</li></ul><figure id="66d7695e-461d-4f9b-a5c6-4b2325558e56" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2028.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2028.png"/></a></figure><ul id="1b5f98b1-a313-4d49-8bb7-95413e40bfc0" class="bulleted-list"><li style="list-style-type:disc">성능</li></ul><figure id="3a9e44aa-cb55-4122-88d5-d0843ec763d6" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2029.png"><img style="width:576px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2029.png"/></a></figure><hr id="ac5fa853-820c-4561-b341-95b476620ddc"/><ul id="745dfa01-67a6-441f-adce-40283ccbac8a" class="bulleted-list"><li style="list-style-type:disc">keras에서 1사이클 스케줄링을 지원하지는 않지만, 매 반복마다 학습률을 조정하는 사용자 정의 콜백을 30줄 미만의 코드로 만들 수 있다.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="cdf03a54-4a85-4066-8f92-6df978f29d28" class="code"><code class="language-Python">import tensorflow.keras.backend as K

class OneCycleScheduler(keras.callbacks.Callback):
    def __init__(self, iterations, max_rate, start_rate=None,
                 last_iterations=None, last_rate=None):
        &quot;&quot;&quot;
        :param iterations: 총 반복 횟수
        :param max_rate: 최대 학습률
        :param start_rate: 시작 학습률 (기본값: 최대 학습률의 1/10)
        :param last_iterations: 마지막 일부 반복 횟수 (기본값: 총 반복 횟수의 1/10 + 1)
        :param last_rate: 마지막 학습률 (기본값: 시작 학습률의 1/1000)
        &quot;&quot;&quot;
        self.iterations = iterations
        self.max_rate = max_rate
        self.start_rate = start_rate or max_rate / 10
        self.last_iterations = last_iterations or iterations // 10 + 1
        self.half_iteration = (iterations - self.last_iterations) // 2
        self.last_rate = last_rate or self.start_rate / 1000
        self.iteration = 0
        
    def _interpolate(self, iter1, iter2, rate1, rate2):
        &quot;&quot;&quot;
        두 점 사이에서 현재 반복 횟수에 따라 학습률을 보간하는 함수
        :param iter1: 시작 반복 횟수
        :param iter2: 종료 반복 횟수
        :param rate1: 시작 학습률
        :param rate2: 종료 학습률
        :return: 보간된 학습률
        &quot;&quot;&quot;
        return ((rate2 - rate1) * (self.iteration - iter1)
                / (iter2 - iter1) + rate1)
                
    def on_batch_begin(self, batch, logs):
        &quot;&quot;&quot;
        각 배치의 학습이 시작되기 전에 호출되는 메서드
        :param batch: 현재 배치 번호
        :param logs: 현재 학습 로그
        &quot;&quot;&quot;
        if self.iteration &lt; self.half_iteration:
            # 반복 횟수가 절반 이전인 경우, 최대 학습률에서 시작 학습률까지 선형적으로 증가
            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)
        elif self.iteration &lt; 2 * self.half_iteration:
            # 반복 횟수가 절반 이후인 경우, 시작 학습률에서 최대 학습률까지 선형적으로 감소
            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,
                                     self.max_rate, self.start_rate)
        else:
            # 마지막 일부 반복인 경우, 시작 학습률에서 아주 작은 값까지 선형적으로 감소
            rate = self._interpolate(2 * self.half_iteration, self.iterations,
                                     self.start_rate, self.last_rate)
            rate = max(rate, self.last_rate)  # 최소값을 설정해줌
        self.iteration += 1
        K.set_value(self.model.optimizer.lr, rate)  # 모델의 학습률 업데이트</code></pre><h1 id="1b46b2ee-f810-468b-831e-1d3c703f4ab3" class="">11.4 규제를 사용해 과대적합 피하기</h1><ul id="2088d38f-8be7-42a7-b589-307d55fc735e" class="bulleted-list"><li style="list-style-type:disc">심층 신경망 = 수많은 파라미터 = 높은 자유도 = 복잡한 데이터셋 학습 가능 = 네트워크를 훈련 세트에 과대적합 되기 쉽게 만듦 = 규제 필요</li></ul><ul id="886236b6-af6c-4043-bc3d-4f4baae046ff" class="bulleted-list"><li style="list-style-type:disc">이미 알아본 규제 방법들<ul id="df0d9e2d-3a26-4f21-92e1-48c490bc011a" class="bulleted-list"><li style="list-style-type:circle">조기 종료</li></ul><ul id="ee6383be-f463-4090-88a1-0e52e25a0a30" class="bulleted-list"><li style="list-style-type:circle">배치 정규화: 불안정한 그레디언트 문제를 해결하기 위해 고안되었지만, 꽤 괜찮은 규제 방법으로 사용될 수 있음.</li></ul></li></ul><h2 id="88542ed8-03dd-4282-873c-9909cb6da553" class="">11.4.1 l1과 l2 규제</h2><ul id="f2ce0298-b637-469a-ba9d-9d3e1ee7a0d2" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">l_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>: (많은 가중치가 0인) 희소 모델 만들기</li></ul><ul id="964dbf84-188d-40d6-8a38-ced2409f9b53" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">l_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>: 신경망의 연결 가중치 제한<figure id="64abce4f-a11c-4e15-acf9-0c60cf6ce968" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2030.png"><img style="width:528px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2030.png"/></a></figure></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="44467d02-144f-41e3-97cb-48a4dc429ec8" class="code"><code class="language-Python">layer = tf.keras.layers.Dense(100, activation=&quot;relu&quot;,
															kernel_initializer=&quot;he_normal&quot;,
															kernel_regularizer=tf.keras.regularizers.l2(0.01))
# keras.regularizers.l1()
# keras.regularizers.l1_l2(): 둘 다 필요할 경우 (ElasticNet)</code></pre><hr id="3cbd560f-696d-47b4-be3e-21cf41d3b5ba"/><ul id="4939d43f-2721-4962-a596-4e4acb80a609" class="bulleted-list"><li style="list-style-type:disc">일반적으로 네트워크의 모든 은닉층에 동일한 ‘활성화 함수’, 동일한 ‘초기화 전략’을 사용하거나 모든 층에 동일한 ‘규제’를 사용함.<ul id="c12f2d09-844c-49a6-84d9-52eed594d9de" class="bulleted-list"><li style="list-style-type:circle">반복문 사용</li></ul><ul id="bd809f5f-a93e-4fa3-82d2-4d2c5df6f610" class="bulleted-list"><li style="list-style-type:circle">functools.partial() 함수를 사용<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bd692499-946d-4e07-9d8d-e3a348c0470e" class="code"><code class="language-Python">
# RegularizedDense를 부분 적용하여 사용하기 위한 함수
RegularizedDense = partial(Dense, activation=&quot;relu&quot;,
													 kernel_initializer=&quot;he_normal&quot;,
													 kernel_regularizer=tf.keras.regularizers.l2(0.01))

# 모델 정의
model = Sequential([
    Flatten(input_shape=(28, 28)),  # 입력 데이터를 28x28의 1차원 벡터로 펼침
    RegularizedDense(100),  # RegularizedDense 레이어 사용하여 100개의 뉴런을 가진 은닉층 생성
    RegularizedDense(100),  # 또 다른 RegularizedDense 레이어를 사용하여 100개의 뉴런을 가진 은닉층 생성
    RegularizedDense(10, activation=&quot;softmax&quot;)  # 마지막 출력층, 10개의 클래스를 분류하는 softmax 활성화 함수 사용
])</code></pre></li></ul></li></ul><h2 id="3aa9965a-f8d3-4132-8576-7404cb716461" class="">11.4.2 드롭아웃</h2><ul id="7888b503-abed-4143-9d82-89a022bcedc5" class="bulleted-list"><li style="list-style-type:disc">매 훈련 스템에서 각 뉴런(입력 뉴런은 포함하고, 출력 뉴런은 제외)은 임시적으로 드롭아웃될 확률 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span><span>﻿</span></span>를 가진다.</li></ul><ul id="aeac4f62-a8e9-4be2-afff-06114c58c201" class="bulleted-list"><li style="list-style-type:disc">이번 훈련 스텝에서 무시된 뉴런들은 다음 스텝에는 활성화될 수 있음.</li></ul><ul id="e4e30d77-9153-40d1-ab17-1b585a0de91b" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span><span>﻿</span></span>: 드롭아웃 비율_dropout rate<ul id="3f69e9fd-79b9-4eb6-a0fd-85e4d835bad0" class="bulleted-list"><li style="list-style-type:circle">보통 10~50%<ul id="e0a3698c-1e33-4b7f-8182-fd226a39d5af" class="bulleted-list"><li style="list-style-type:square">RNN: 20~30%</li></ul><ul id="590c158f-3b52-4acb-9d31-ffe1236cea04" class="bulleted-list"><li style="list-style-type:square">CNN: 40~50%</li></ul></li></ul></li></ul><hr id="f76650fb-4563-4675-83e3-891311a9000d"/><figure id="94163f80-719c-4454-b55c-3c5058e97d77" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2031.png"><img style="width:624px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2031.png"/></a></figure><hr id="9eb14998-1b8e-4496-8e99-f4b4d44b2e4c"/><ul id="bf82fef7-229e-4893-9672-77ce884b08b8" class="bulleted-list"><li style="list-style-type:disc">드롭아웃의 이해 (1)<ol type="a" id="b41e19fa-fc60-412c-b75f-8faba9bb26f8" class="numbered-list" start="1"><li>드롭아웃으로 훈련된 뉴런은 이웃한 뉴런에 맞추어 적응할 수 없다.<p id="7a4b27ba-1010-4b71-aed8-fbc2d086baeb" class="">즉, 가능한 한 자기 자신이 유용해져야 하는 것이다.</p></li></ol><ol type="a" id="71dfe00e-c1e4-4388-847c-7ee10adc10d1" class="numbered-list" start="2"><li>그리고, 몇 개의 입력 뉴런에만 지나치게 의존할 수도 없다.<p id="d994b514-c979-4f94-9477-076a8e019fa6" class="">즉, 모든 입력 뉴런에 주의를 기울여야한다.</p></li></ol><ol type="a" id="3d0aca50-dfb8-4409-9cc3-18d8b976e86f" class="numbered-list" start="3"><li>그러므로 <span style="border-bottom:0.05em solid">입력값의 작은 변화에 덜 민감해진다.</span></li></ol><ol type="a" id="d83897e2-31b4-405d-95e5-999d8a640b3f" class="numbered-list" start="4"><li>결국 더 안정적인 네트워크가 되어 일반화 성능이 좋아진다.</li></ol></li></ul><ul id="36e47592-0923-4168-87b5-e774f40ff288" class="bulleted-list"><li style="list-style-type:disc">드롭아웃의 이해 (2)<ol type="a" id="4c802a03-db0a-43b1-a13b-a4c50c004525" class="numbered-list" start="1"><li>각 훈련 스텝에서 고유한 네트워크가 생성된다고 생각하자.</li></ol><ol type="a" id="e4054fbd-1810-4ff8-b6fe-202f92e0dfd8" class="numbered-list" start="2"><li>10,000번의 훈련 스텝을 진행하면 10,000개의 다른 신경망을 훈련하게 된다.</li></ol><ol type="a" id="7b9673aa-1948-47bd-9ba4-01bbb26b5c7e" class="numbered-list" start="3"><li>이 신경망은 대부분의 가중치를 공유하고 있기 때문에 아주 독립적이지 않다.</li></ol><ol type="a" id="8e9e3214-7b1d-4237-b219-5fedbc665cc9" class="numbered-list" start="4"><li>하지만 그럼에도 모두 다르다.</li></ol><ol type="a" id="008ea4d3-b2bf-4c84-b086-0ad1754ee3b4" class="numbered-list" start="5"><li>결과적으로, 만들어진 신경망은 이 모든 <span style="border-bottom:0.05em solid">‘신경망을 평균한 앙상블’로 볼 수 있다.</span></li></ol></li></ul><hr id="3c386270-bfcc-4af1-8754-128e6e2cf2da"/><ul id="d25174c4-515c-476a-a90b-6b70631f4f93" class="bulleted-list"><li style="list-style-type:disc">일반적으로 (출력 층을 제외한) 맨 위층부터 3번째 층까지의 뉴런에만 드롭아웃을 적용한다.</li></ul><hr id="adc88fa5-ca78-4da0-a281-f5fce6c805fe"/><ul id="82d3223d-f200-4377-aa8d-4db1db47ab89" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span><span>﻿</span></span> = 75%로 하면, 평균적으로 뉴런의 25%만 훈련하는 동안 작동한다.</li></ul><ul id="b0c1c5d0-edce-49c2-8971-667eaf65aedc" class="bulleted-list"><li style="list-style-type:disc">훈련이 끝나면 각 뉴런이 훈련할 때보다 4배가 많은 입력에 연결되는 것이다.</li></ul><ul id="a0672651-44e9-48cc-8666-3d408864b705" class="bulleted-list"><li style="list-style-type:disc">이런 점을 보상하기 위해, 각 뉴런의 연결 가중치에 4를 곱해야 한다.<ul id="7663c1c0-cc85-4e6c-9fde-4ca203401140" class="bulleted-list"><li style="list-style-type:circle">그렇지 않으면, 훈련 중 &amp; 훈련 후에 각각 다른 데이터를 만나게 되므로 제대로 작동하지 않을 것이다.</li></ul></li></ul><ul id="4ce3bd43-d240-4881-b80b-9ad609c894e1" class="bulleted-list"><li style="list-style-type:disc">일반적으로, 훈련하는 동안 <span style="border-bottom:0.05em solid">각 입력 연결 가중치를 ‘보존 확률(keep probability)’ </span><span style="border-bottom:0.05em solid"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1-p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span></span><span style="border-bottom:0.05em solid">로 나눠야</span> 한다.</li></ul><hr id="ef101321-b326-42be-a139-03a3668d9663"/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b09f155b-06c3-4530-8dce-cfad1aff0555" class="code"><code class="language-Python">model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),  # 입력 데이터를 28x28의 1차원 벡터로 펼침
    
    tf.keras.layers.Dropout(0.2),  # 드롭아웃 비율이 0.2인 드롭아웃 레이어 추가
    tf.keras.layers.Dense(100, activation=&#x27;relu&#x27;, kernel_initializer=&#x27;he_normal&#x27;),  # 100개의 뉴런을 가진 ReLU 활성화 함수와 He 초기화를 사용하는 fully connected layer 추가
    
    tf.keras.layers.Dropout(0.2),  # 드롭아웃 비율이 0.2인 드롭아웃 레이어 추가
    tf.keras.layers.Dense(100, activation=&#x27;relu&#x27;, kernel_initializer=&#x27;he_normal&#x27;),  # 또 다른 100개의 뉴런을 가진 ReLU 활성화 함수와 He 초기화를 사용하는 fully connected layer 추가
    
    tf.keras.layers.Dropout(0.2),  # 드롭아웃 비율이 0.2인 드롭아웃 레이어 추가
    tf.keras.layers.Dense(10, activation=&#x27;softmax&#x27;)  # 출력층, 10개의 클래스를 분류하는 softmax 활성화 함수 사용
])</code></pre><hr id="034b7fe4-3692-4ca9-bc94-c8a1c0a9accf"/><ul id="bd1dd547-fa4d-47bf-a939-5eb4c3cdff61" class="bulleted-list"><li style="list-style-type:disc">드롭아웃 비율<ul id="eb29a22b-db96-4502-8eb3-7d73a9569098" class="bulleted-list"><li style="list-style-type:circle">드롭아웃 비율 UP<ul id="3a4d29d7-7f4c-43fa-8603-bd9d61a265af" class="bulleted-list"><li style="list-style-type:square">모델이 과대적합 된 경우</li></ul><ul id="ba0c2313-d685-48b2-88d4-ad59e04e7333" class="bulleted-list"><li style="list-style-type:square">층이 클  때</li></ul></li></ul><ul id="1eb31aaf-a8b4-4ddb-8a44-916bc9f99092" class="bulleted-list"><li style="list-style-type:circle">드롭아웃 비율 DOWN<ul id="18f57d34-c7de-4156-acb7-5adb77532a01" class="bulleted-list"><li style="list-style-type:square">모델이 훈련 세트에 과소 적합된 경우</li></ul></li></ul><ul id="f700c584-1fe0-4a0c-bfee-31bb55ca4a27" class="bulleted-list"><li style="list-style-type:circle">층이 작을 때</li></ul></li></ul><ul id="99f15757-d7d2-4e81-b9da-18b6ba3ed2f0" class="bulleted-list"><li style="list-style-type:disc">최신 신경망 구조는 마지막 은닉층 뒤에만 드롭아웃을 사용.</li></ul><ul id="363524a1-7821-467f-9092-85d40e3d0e37" class="bulleted-list"><li style="list-style-type:disc">드롭아웃은 수렴을 느리게 만들지만, 적절하게 튜닝하면 더 좋은 모델을 만들기에 대규모 모델일 경우 추가적인 시간과 노력을 기울일 가치가 있다.</li></ul><hr id="4cf3bb65-3ee4-4a22-ba7e-ea295239a475"/><figure id="3ad8f82c-2ceb-4190-8f02-f62055f45487" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2032.png"><img style="width:624px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2032.png"/></a></figure><h2 id="d7cbdf45-16e9-4ab0-9000-18b40f5db6f9" class="">11.4.3 몬테 카를로 드롭아웃</h2><ul id="4e8542c1-f529-46a0-b675-82c47cff0f05" class="bulleted-list"><li style="list-style-type:disc">몬테 카를로 드롭아웃(MC 드롭아웃): 훈련된 드롭아웃 모델을 재훈련하거나 전혀 수정하지 않고 성능을 크게 향상시킬 수 있음.<ul id="2fc40e60-e224-414f-bbde-71db8bef80ae" class="bulleted-list"><li style="list-style-type:circle">모델의 불확실성을 더 잘 측정할 수 있음.</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f63a0e45-fb0d-488a-bffb-afb69710ee54" class="code"><code class="language-Python">import numpy as np

y_probas = np.stack([model(X_test, training=True) # training=True로 Dropout 층이 활성화됨.
										for sample in range(100)])
 # 테스트 세트에서 100개의 예측을 만들고 평균을 계산.
y_proba = y_probas.mean(axis=0)</code></pre><h2 id="0e3c9090-f399-4331-af43-75bb8072b0d4" class="">11.4.4 맥스-노름 규제</h2><ul id="915f9719-6c23-479f-9b08-83368e2c7766" class="bulleted-list"><li style="list-style-type:disc">각각의 뉴런에 대해 입력의 연결 가중치 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span><span>﻿</span></span>가 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mi>w</mi><msub><mo stretchy="false">∥</mo><mn>2</mn></msub><mo>≤</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">\lVert w \rVert_2 \le r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∥</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose"><span class="mclose">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span></span><span>﻿</span></span>이 되도록 제한한다.<ul id="ca6601e6-b471-4891-a92b-bf6a15162dd9" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span></span><span>﻿</span></span>: 맥스-노름 하이퍼파라미터<ul id="418fbd06-432b-40b8-82c9-3df37c9cc3d3" class="bulleted-list"><li style="list-style-type:square"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span></span><span>﻿</span></span>을 줄이면 규제 양이 증가해서 과대적합을 감소시키는 데 도움.</li></ul><ul id="4d04b7b8-2b90-4cb3-8f7e-88f828d8836e" class="bulleted-list"><li style="list-style-type:square">맥스-노름 규제는 (배치 정규화 사용 안했을 때) 불안정한 그레디언트 문제를 완화하는 데 도움.</li></ul></li></ul><ul id="954f4689-11d7-46b6-86ea-946b492e5782" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mo>⋅</mo><msub><mo stretchy="false">∥</mo><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\lVert \cdot \rVert_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∥</span><span class="mord">⋅</span><span class="mclose"><span class="mclose">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> 는 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">l_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> 노름을 나타낸다.</li></ul></li></ul><ul id="b3efbb2e-38ac-4a33-8b55-5c561f3e7a3c" class="bulleted-list"><li style="list-style-type:disc">맥스-노름 규제는 전체 손실함수에 ‘규제 손실항’을 추가하지 않는다.<ul id="4fabec9b-cf8e-46d0-a0a2-e33d6ebeb764" class="bulleted-list"><li style="list-style-type:circle">대신 일반적으로 매 훈련 스텝이 끝나고 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mi>w</mi><msub><mo stretchy="false">∥</mo><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\lVert w \rVert_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∥</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose"><span class="mclose">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> 를 계산하고, 필요하면 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span><span>﻿</span></span>의 스케일을 조정한다. (<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>←</mo><mi>w</mi><mfrac><mi>r</mi><mrow><mo stretchy="false">∥</mo><mi>w</mi><msub><mo stretchy="false">∥</mo><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">w \leftarrow w\frac{r}{\lVert w \rVert_2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2154em;vertical-align:-0.52em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">∥</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mclose mtight"><span class="mclose mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span><span>﻿</span></span>)</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="49480f41-fa5f-49e9-95e5-551c436b3ba7" class="code"><code class="language-Python">dense = tf.keras.layers.Dense(
		100, activation=&quot;relu&quot;, kernel_initializer=&quot;he_normal&quot;,
		kernel_constraint=tf.keras.constraints.max_norm(1.))</code></pre><ul id="5de5e538-9b66-4172-802a-1451c0147ba9" class="bulleted-list"><li style="list-style-type:disc">max_norm()은 기본값이 0인 axis 매개변수를 가진다.<ul id="75b97cb3-c47b-4daf-85fa-db01afac1ea4" class="bulleted-list"><li style="list-style-type:circle">Dense 층은 일반적으로 [샘플 개수, 뉴런 개수] 크기의 가중치를 가진다.</li></ul><ul id="b8b49b5d-05a1-4b79-abe6-09b3c74b6c28" class="bulleted-list"><li style="list-style-type:circle">axis=0을 사용하면 맥스-노름 규제는 각 뉴런의 가중치 벡터에 독립적으로 적용된다.</li></ul><ul id="46c5eebc-9dcd-420e-8d3d-18d85623953a" class="bulleted-list"><li style="list-style-type:circle">Convolution layer는 일반적으로 [샘플 개수, 높이, 너비, 채널 개수] 크기의 가중치를 가지기 때문에, axis=[0,1,2]로 지정하면 채널축에 독립적으로 적용된다.</li></ul></li></ul><h2 id="c3c4a591-5128-4c47-b575-1532412fc82a" class="">11.5 요약 및 실용적인 가이드라인</h2><figure id="e9f4f007-5ff1-40c8-9190-caeec594e198" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2033.png"><img style="width:432px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2033.png"/></a></figure><figure id="eabe1a4a-a081-4f2c-8fca-242f43bc8c9d" class="image"><a href="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2034.png"><img style="width:432px" src="11%20%E1%84%89%E1%85%B5%E1%86%B7%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%92%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB%20a9b5a561a9e149b68f06f45841b15bc6/Untitled%2034.png"/></a></figure><h1 id="4193289d-a861-4263-8ec8-a2a8807ae850" class="">연습문제</h1><h3 id="65c8e0e8-1ed4-475b-8ff3-315e2430e704" class="">1) 글로럿 초기화와 He 초기화가 해결하고자 하는 문제는 무엇인가요?</h3><h3 id="db55ae89-bf25-470e-a3ab-1ffe3bef794c" class="">2) He 초기화를 사용하여 랜덤으로 선택한 값이라면 모든 가중치를 같은 값으로 초기화해도 괜찮을까요?</h3><h3 id="cce6fc89-090f-4e64-bdd3-4d4dcdcac357" class="">3) 편향을 0으로 초기화해도 괜찮을까요?</h3><h3 id="c32a3fa4-2deb-4ef7-9a95-9faecc6f94cf" class="">4) 어떤 경우에 이 장에서 언급한 활성화 함수를 사용하나요?</h3><h3 id="5e7c408c-12c3-4535-8773-59e1fb6b9b58" class="">5) SGD 옵티마이저를 사용할 때 momentum 하이퍼파라미터를 너무 1에 가깝게 하면(ex. 0/99999) 어떤 일이 일어날까요?</h3><h3 id="d5a4b61c-647f-4f05-a21f-db68d6b3568d" class="">6) 희소 모델을 만들 수 있는 3가지 방법은 무엇인가요?</h3><h3 id="3ebdb317-e1df-405b-b228-0ce6bb556f90" class="">7) 드롭아웃이 훈련 속도를 느리게 만드나요? 추론(새로운 샘플에 대한 예측을 만드는 것)도 느리게 만드나요? MC 드롭아웃은 어떤가요?</h3><h3 id="2c9c8fce-976b-40b5-955c-7eebf61b2136" class="">8) CIFAR10 이미지 데이터셋에 심층 신경망을 훈련해보세요.</h3><ol type="a" id="f2a1def2-fa83-43ed-862c-975881140ce1" class="numbered-list" start="1"><li>100개 뉴런을 가진 은닉 층 20개로 심층 신경망을 만들어보세요(너무 많은 것 같지만 이 연습문제의 핵심입니다). He 초기화와 Swish 활성화 함수를 사용하세요.</li></ol><ol type="a" id="134ef4a9-6684-4031-8a0e-b94a43bfe94e" class="numbered-list" start="2"><li>Nadam 옵티마이저와 조기 종료를 사용하여 CIFAR10 데이터셋에 이 네트워크를 훈련하세요. tf.keras.datasets.cifar10.load_data()를 사용하여 데이터를 적재할 수 있습니다. 이 데이터셋은 10개의 클래스와 32x32 크기의 컬러 이미지 60,000개로 구성됩니다.(50,000개는 훈련, 10,000개는 테스트), 따라서 10개의 뉴런과 소프트맥스 활성화 함수를 사용하는 출력 층이 필요합니다. 모델 구조와 하이퍼파라미터를 바꿀 때마다 적절한 학습률을 찾아야 한다는 것을 기억하세요.</li></ol><ol type="a" id="d7eb971c-859e-4b9b-81ad-a44a1005a883" class="numbered-list" start="3"><li>배치 정규화를 추가하고 학습 곡선을 비교해보세요. 이전보다 빠르게 수렴하나요? 더 좋은 모델이 만들어지나요? 훈련 속도에는 어떤 영향을 미치나요?</li></ol><ol type="a" id="8f491bb2-4cbe-4208-b966-4a26d6ea4611" class="numbered-list" start="4"><li>배치 정규화를 SELU로 바꾸어보세요. 네트워크가 자기 정규화하기 위해 필요한 변경 사항을 적용해보세요 (입력 특성 표준화, 르쿤 정규 분포 초기화, 완전 연결 층만 순차적으 로 쌓은 심층 신경망 등).</li></ol><ol type="a" id="755fa203-8014-4a73-ba52-7cdf58d1b94c" class="numbered-list" start="5"><li>알파 드롭아웃으로 모델에 규제를 적용해보세요. 그다음 모델을 다시 훈련하지 않고 MC 드롭아웃으로 더 높은 정확도를 얻을 수 있는지 확인해보세요.</li></ol><ol type="a" id="aa3d6834-1570-476c-8888-13f8fb5b8162" class="numbered-list" start="6"><li>1사이클 스케줄링으로 모델을 다시 훈련하고 훈련 속도와 모델 정확도가 향상되는지 확 인해보세요.</li></ol><p id="0cb16d83-7d98-416a-b16c-767bfa42ea50" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>